---
layout: post
title: 6주차 ML 비지도학습 WIL
data: 2021-10-31 23:00:00 +0900
author: goldtan
categories: ["1st_term"]
tags: ["ml"]
---

# 3 비지도학습과 데이터 전처리
<br>

# 3.5 군집
## 3.5.1 K-means clustering
<br>
<p>
K-means clustering은 가장 간단하고 널리 사용되는 군집 알고리즘입니다.<br>
알고리즘은 크게 2 Step으로 나뉘는데, 먼저 데이터 포인트를 가장 가까운 클러스터 중심에 할당하고 다시 할당된 데이터 포인트들의 평균으로 클러스터 중심을 이동시킵니다.<br>
이후 변화가 없을 때 알고리즘은 종료가 됩니다. 
</p>
<br>

**k-평균 알고리즘이 실패하는 경우** 
<br>
<p>
K-means clustering은 원형이 아닌 복잡한 형태의 클러스터를 잘 구분하지 못합니다. 
</p>
<br>

**벡터 양자화 또는 분해 메서드로서의 k-means** 
<br>
<p> 
K-means clustering에서는 각 클러스터 중심으로 각 데이터 포인트를 표현하는데, 이는 역으로 각 데이터 포인트가 클러스터 중심으로 표현된다고도 볼 수 있습니다.<br>
이 때, K-means를 각 포인트가 하나의 성분(클러스터의 중심)으로 분해된다고 보는 관점을 벡터 양자화라고 합니다.<br>
K-means를 사용한 벡터 양자화의 흥미로운 점은 입력 데이터의 차원보다 더 많은 클러스터를 사용해 데이터를 인코딩 할 수 있다는 점입니다.<br>
차원 축소 알고리즘의 경우 차원이 낮을 경우 더 이상 차원을 축소할 경우 데이터가 크게 손상되기 때문에 할 수 있는 부분이 제한되어 있지만, k-means는 이 부분에서 자유로울 수 있습니다.
</p>
<br>
<p>
K-means의 장점은 비교적 이해하기 쉽고, 구현도 쉽고 빠르다는 것입니다.<br>
반면 단점은 결과가 처음 클러스터의 중심이 랜덤으로 생성되는 것에 따라 달라지는 것과 클러스터의 모양을 가정하고 있어서 활용 범위가 제한적이고, 찾으려 하는 클러스터의 개수를 직접 정해줘야 한다는 것입니다.
</p>

---
---

## 3.5.2 병합 군집
<br>
<p>
병합 군집은 시작할 때는 각 데이터 포인트가 하나의 클러스터가 됩니다. <br> 
이후 특정 조건을 만족하기 전까지 가장 비슷한 클러스터들(데이터 포인트들)을 합쳐가는 방식을 통해 클러스터를 형성합니다.<br>
scikit-learn에서는 특정 조건을 클러스터 개수로 지정하여, 지정한 클러스터 수가 될 때까지 반복합니다.<br>
</p>
<br>
<p>
linkage 옵션 <br> 
가장 비슷한 클러스터를 측정한다고 할 때, 비슷함을 측정하는 기준을 의미합니다.<br>
ward : 분산을 최소화 (default)<br>
average : 두 세트 사이의 평균 거리<br>
complete(or maximum) : 두 세트 사이의 최대 거리<br>
single : 두 세트 사이의 최소 거리<br>
</p>
<br>


**계층적 군집과 덴드로그램**
<br> 
<p>
병합 군집은 계층적 군집을 만듭니다.<br>
데이터 포인트들을 합쳐가는 과정을 그림으로 표현하면 계층 군집의 모습을 나타낼 수 있습니다.<br>
하지만 이는 2차원으로만 표현이 가능하기 때문에 우리는 덴드로그램을 사용합니다.<br>
덴드로그램에서는 맨 밑에 데이터 포인트들이 위치하고 이 포인트들이 잎이 되는 트리가 만들어집니다.<br>
(Decision Tree와 다르게 밑에서부터 위로 그리는 방식)<br>
가지의 길이는 합쳐진 클러스터가 얼마나 멀리 떨어져 있는지를 보여줍니다.
</p>
<br>

![dendro](https://user-images.githubusercontent.com/83542989/139846216-7d4a952c-e102-429e-8699-be7aab00a7bb.png)

<br>
병합 군집도 복잡한 형상을 구분하지 못한다는 단점이 있습니다.

---
---

## 3.5.3 DBSCAN
<br>
<p>
DBSCAN은 데이터가 많아 붐비는 지역의 포인트를 찾고, 이를 밀집 지역이라고 합니다.<br>
데이터가 밀집되어 있는 공간이 한 클러스터가 되고 비교적 밀도가 낮은 공간이 경계가 된다는 것이 알고리즘의 아이디어입니다.<br>
매개변수는 min_samples와 eps가 있는데, 한 데이터 포인트에서 eps 거리 안에 데이터 포인트가 min_samples 만큼 있으면 이 데이터 포인트는 핵심 샘플이 됩니다.<br>
이후 마치 파도타기처럼 핵심 샘플 이웃에 있는(eps 거리 안) 포인트들이 핵심 샘플인지 확인합니다.<br>
이러한 반복 작업을 마치면 포인트는 핵심 포인트, 경계 포인트, 잡음 포인트 총 세 가지로 나뉩니다.<br>
이 때, 경계 포인트는 핵심 샘플을 통해 방문하는 순서에 따라 달라질 수 있지만 보통 숫자가 적고 영향이 적어 중요하지 않습니다.<br>

![parameter](https://user-images.githubusercontent.com/83542989/139846041-c36a545c-969f-4234-a0aa-b56202ae6119.png)


<br>
DBSCAN의 주요 장점은 클러스터의 개수를 미리 지정하지 않아도 됩니다.<br>
그리고 복잡한 형상을 찾을 수 있으며, 어느 클래스에도 속하지 않은 포인트를 구분할 수 있습니다.
</p>
<br>

---
---

## 3.5.4 군집 알고리즘의 비교와 평가
**타깃 값으로 군집 평가** 
<br>
<p>
타깃 값(정답)이 있을 때는 ARI와 MNI라는 지표를 사용할 수 있습니다.<br>
ARI는 무작위일 경우 0이 되고 무작위보다 낮을 경우 음수가 됩니다.<br>
NMI는 상호 의존성을 측정하며, 엔트로피의 개념과 밀접하다고 합니다.<br>
둘은 모두 높은 성능일수록 1에 가까운 값이 나오게 됩니다.
</p>
<br>

**타깃 값 없이 군집 평가** 
<br>
<p> 
타깃 값이 없을 때는 실루엣 계수를 사용합니다.<br>
하지만 실루엣 계수는 클러스터의 밀집 정도를 반영하는 것이기 때문에 모양이 복잡할 때는 적합하지 않습니다.
</p>


---
---

## 3.5.5 군집 알고리즘 요약
<br>
<p>
k-means와 병합 군집은 원하는 클러스터 개수를 지정할 수 있고, DBSCAN도 매개변수 튜닝을 통해 간접적으로 클러스터의 크기를 조절할 수 있습니다.<br>
k-means는 각 데이터포인트를 클러스터의 centroid로 표현할 수 있기 때문에 분해 방법으로도 사용될 수 있다.<br>
DBSCAN은 잡음 포인트를 발견할 수 있고, 클러스터의 개수도 지정하지 않아도 됩니다. 그리고 다른 두 방법과 달리 복잡한 클러스터의 모양도 해결할 수 있습니다.<br>
병합 군집은 전체 데이터의 분할 계층도를 만들며 덴드로그램을 사용할 수 있기 때문에 클러스터의 개수를 정하기에 용이할 수 있습니다.
</p>
<br>

---
---
---

<br>

# 마무리

중간고사 기간이 끝났으니 다시 달려보겠습니다 !<br>
### GDSC 화이팅 ! ML 화이팅 !