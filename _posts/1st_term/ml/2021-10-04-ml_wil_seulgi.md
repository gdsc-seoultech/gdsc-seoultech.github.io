---
layout: post
title: 4주차 ML WIL
date: 2021-10-03 23:00:00 +0900
author: ssggi
categories: ["1st_term"]
tags: ["ml"]
---

# 결정 트리

결정트리는 분류와 회귀 문제에 널리 사용하는 모델
예/아니오 질문을 이어나가며 학습한다
<br>
- **노드** : 질문이나 정답을 담은 네모 상자. 맨 위 노드는 **루트 노드**라 불리고, 마지막 노드는 **리프**라고도 한다. 
- **에지**는 질문의 답과 다음 질문을 연결한다.
<br>

> ### 결정트리 만들기

- 데이터셋 : two_moons 사용
<br>
- 결정 트리를 학습한다는 것 -> 정답에 가장 빨리 도달하는 예/아니오 질문 목록을 학습한다는 것  
- 머신러닝에서 이런 질문들을 **테스트**라고 한다.  

---
가장 좋은 테스트를 찾는 과정을 반복해서 모델을 더 정확하게 만들 수 있다.

- 반복된 프로세스는 각 노드가 테스트 하나씩을 가진 **이진 결정 트리**를 만든다 (True/False)  
- 즉 각 테스트는 하나의 축을 따라 데이터를 둘로 나눈다  
- 계층적으로 영역을 분할해가는 알고리즘이며 각 테스트는 하나의 특성에 대해서만 이루어지므로 나누어진 영역은 항상 축에 평행  

- 데이터를 분할하는 것은 각 분할된 영역이 (결정 트리의 리프) 한 개의 타깃값(하나의 클래스나 회귀 분석 결과)을 가질 때까지 반복!

- 타깃 하나로만 이루어진 리프 노드를 **순수 노드**라고 한다.

---

새로운 데이터 포인트에 대한 예측은 주어진 데이터 포인트가 특성을 분할한 영역들 중 어디에 놓이는지를 확인,  
그 영역의 타깃 값 중 다수인 것을 예측 결과로 한다. 
_순수 노드라면 하나!_
<br>
- **회귀** 문제에도 트리 사용 가능  
같은 방법으로 새로운 데이터 포인트에 해당되는 리프 노드를 찾고, 찾은 리프 노드의 훈련 데이터 평균값이 이 데이터 포인트의 출력
<br>

>### 결정 트리의 복잡도 제어하기

과대적합을 막는 전략

1. **사전 가지치기** : 트리의 최대 깊이나 리프의 최대 개수를 제한하거나, 노트가 분할하기 위한 포인트의 최소 개수를 지정
2. **사후 가지치기** : 트리를 만든 후 데이터 포인트가 적은 노드를 사용하거나 병합
<br>

>### 트리의 특성 중요도

전체 트리를 살펴보는 것 대신, 트리가 어떻게 작동하는지 요약하는 속성을 사용할 수 있음
- **특성 중요도** : 각 특성이 트리를 만드는 결정에 얼마나 중요한지를 평가.
0과 1 사이의 숫자로 1에 가까울 수록 중요한 특성.
특성 중요도의 전체 합은 1.

- 분류 트리와 달리 회귀 결정 트리 DecisionTreeRegressor는 **외삽**, 즉 훈련 데이터의 범위 밖의 포인트에 대해 예측할 수 없다.
<br> 

>### 장단점과 매개변수

결정 트리에서 모델 복잡도를 조절하는 매개변수는 사전 가지치기 매개변수 (사이킷런은 사전 가지치기만 지원)
- max_depth, max_leaf_nodes, min_samples_leaf

결정 트리의 장점
- 만들어진 모델을 쉽게 시각화할 수 있음
- 데이터의 스케일의 영향을 받지 않아 특성의 정규화나 표준화 같은 전처리 과정 필요 없음
- 스케일이 서로 다르거나 특성이 혼합되어 있을 때도 잘 작동

주요 단점
- 사전 가지치기를 사용함에도 불구하고 과대적합 되는 경향
<br>

# 결정 트리의 앙상블

**앙상블** : 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법  
<br>
분류와 회귀 모델의 다양한 데이터셋에서 효과적인 모델 
--> **랜덤 포레스트**와 **그레이디언트 부스팅**~!!  
<br>

> ### 랜덤 포레스트

- 랜덤 포레스트는 기본적으로 조금씩 다른 여러 결정 트리의 묶음  
- 잘 작동하되 서로 다른 방향으로 과대적합된 트리를 많이 만들고, 그 결과를 평균냄으로써 과대적합된 양을 줄일 수 있다.  
--> 예측 성능 유지, 과대적합 줄임  

- 트리 생성 시 트리들이 달라지도록 무작위성을 주입.  
1. 트리를 만들 떄 사용하는 **데이터 포인트**를 무작위로 선택
2. 분할 테스트에서 **특성**을 무작위로 선택  
<br>

> ### 랜덤 포레스트 구축

생성할 트리의 개수를 정한다. - n_estimators 매개변수  

각 트리를 완전히 독립적으로 만들기 위해 먼저 데이터의 **부트스트랩 샘플**을 생성.  
(n_samples개의 데이터 포인트 중에서 무작위로 데이터를 n_samples 횟수만큼 반복 추출한다)  
<br>
특성의 개수는 max_feature 매개변수로 조정 가능  

이렇게 만든 데이터셋으로 결정트리를 만듦  
- 부트스트랩 샘플링을 통해 랜덤 포레스트의 트리가 조금씩 다른 데이터셋으로 만들어지게 됨  
- 각 노드에서 특성의 일부만 사용하기 때문에 트리의 각 분기는 각기 다른 특성의 부분 집합을 사용  
--> 랜덤 포레스트의 모든 트리가 서로 달라짐  

**매개변수 max_features**  
max_features의 값을 크게 하면 트리들은 매우 비슷해지고 가장 두드러진 특성을 이용해 데이터에 잘 맞춰짐  
값을 낮추면 트리들은 많이 달라지고 각 트리는 데이터에 맞추기 위해 깊이가 깊어짐  
<br>

> ### 장단점과 매개변수

- 성능이 매우 뛰어나고 매개변수 튜닝을 많이 하지 않아도 잘 작동하며 데이터의 스케일을 맞출 필요가 없다  
- 텍스트 데이터처럼 차원이 높고 희소한 데이터에는 잘 작동하지 않음  
- 선형 모델보다 많은 메모리를 사용하며 훈련과 예측이 느림  
<br>
- 중요 매개변수는 n_estimators, max_features, max_depth같은 사전 가지치기 옵션  <br>
- n_estimators는 클수록 좋다. 
--> 더 많은 트리를 평균하면 과대적합 줄여 더 안정적인 모델 생성 but 긴 훈련 시간 ㅜㅜ  
<br>

> ### 그래디언트 부스팅 회귀 트리  

여러 개의 결정 트리를 묶어 강력한 모델을 만드는 또 다른 앙상블 방법  
(회귀와 분류 모두에 사용 가능)  

- 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듦  
- 무작위성이 없는 대신 사전 가지치기가 사용됨  
- 메모리를 적게 사용하고 예측이 빠른 것이 특징  
<br>
- 얕은 트리 같은 간단한 모델(**약한 학습기**)을 많이 연결하는 것이 근본적인 아이디어  
- 각각의 트리는 일부 데이터에만 예측을 잘 수행할 수 있어 트리가 많이 추가될수록 성능이 좋아짐  
**learning rate(학습률)**  
- 학습률이 크면 트리는 오차를 강하게 보정, 복잡한 모델 생성  
<br>

> ### 장단점과 매개변수

- 매개변수를 잘 조절해야한다는 것과 훈련 시간이 길다는 단점이 있음
<br>
- 중요 매개변수는 **n_estimators**(트리의 개수를 지정)와 **learning_rate**(트리의 오차를 보정하는 정도를 조절) 
- n_estimators가 클수록 모델이 복잡해지고 과대적합될 가능성이 높아짐 
- learning_rate를 낮추면 비슷한 복잡도의 모델을 많이 만들기 위해 더 많은 트리를 추가해야함 - 뭔 말이지?
- max_depth 또는 max_leaf_nodes (각 트리의 복잡도를 낮춤)  
<br>

### 배깅, 엑스트라 트리, 에이다부스트

> ### 배깅

Bootstrap aggregating의 줄임말  
중복을 허용한 랜덤 샘플링으로 만든 훈련 세트를 사용하여 분류기를 각기 다르게 학습시킴  
- 분류기가 predict_proba() 메서드를 지원하는 경우 확률값을 평균하여 예측을 수행함
- 그렇지 않은 분류기에서는 가장 빈도가 높은 클래스 레이블이 예측 결과가 됨
<br>

> ### 엑스트리 트리

랜덤 포레스트와 비슷하지만 후보 특성을 무작위로 분할한 다음 최저의 분할을 찾음  
- DecisionTreeClassifier 사용, 부트스트랩 샘플링은 적용하지 않음
- 무작위성을 증가시키면 모델의 편향 증가, 분산 감소
- 예측 방식: 각 트리가 만든 확률값을 평균함
- 일반적으로 랜덤 포레스트가 더 선호된다.
: 엑스트라 트리가 랜덤 포레스트보다 계산 비용이 적지만 무작위 분할 때문에 일반화 성능을 높이려면 많은 드리를 만들어야 함
<br>

> ### 에이다부스트

Adaptive Boostion의 줄임말  
약한 학습기를 사용, 이전의 모델이 잘못 분류한 샘플에 가중치를 높여서 다음 모델을 훈련시킴
- 훈련된 각 모델은 성능에 따라 가중치가 부여됨
- 예측 방식: 모델이 예측한 레이블을 기준으로 모델의 가중치를 합산하여 가장 높은 값을 가진 레이블을 선택!


# 커널 서포트 벡터 머신
입력 데이터에서 단순한 초평면으로 정의되지 않는 더 복잡한 모델을 만들 수 있도록 확장한 것

> ### 선형 모델과 비선형 특성

선형 모델을 유연하게 만드는 방법은 특성끼리 곱하거나 특성을 거듭제곱하는 식으로 새로운 특성을 추가하는 것

> ### 커널 기법

실제로 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리를 계산
- SVM에서 데이터를 고차원 공간에 매핑하는 법
1. 원래 특성의 가능한 조합을 지정된 차수까지 모두 계산하는 **다항식 커널**
2. 가우시안 커널로도 불리는 **RBF 커널** 
: 차원이 무한한 특성 공간에 매핑하는 것으로 모든 차수의 모든 다항식을 고려함 
특성의 중요도는 고차항이 될수록 줄어듦(지수 함수의 테일러 급수 전개 때문 - ???)

> ### SVM 이해하기

일반적으로 두 클래스 사이의 경계에 위치한 훈련 데이터의 일부만 결정 경계를 만드는 데 영향을 준다. 이런 데이터 포인트를 **서포트 벡터**라고 한다.

새로운 데이터 포인트에 대한 분류 결정은 서포트 벡터까지의 거리에 기반, 서포트 벡터의 중요도는 훈련 과정에서 학습!

> ### SVM 매개변수 튜닝

1. gamma 매개변수
   - rambda로, 가우시안 커널 폭의 역!수!
   - 하나의 훈련 샘플이 미치는 영향의 범위를 결정
   - 가우시안 커널의 반경이 클수록 훈련 샘플의 영향 범위도 커짐  

 2. C 매개변수
    - 규제 매개변수. 각 포인트의 중요도를 제한함.
    
3. 정리
- gamma 매개변수가 커지면 모델 복잡도 증가
- 작은 C 매개변수는 매우 제약이 큰 모델을 만들고 각 데이터 포인트의 영향력이 작음
- C 매개변수가 커지면 더 복잡한 모델을 만듦

   *gamma, C 둘 다 값이 커지면 모델이 복잡해지는 거임*

    
> ### SVM을 위한 데이터 전처리

- 모든 특성 값을 평균이 0이고 단위 분산이 되도록 하거나, 0과 1 사이로 맞추는 방법을 많이 사용
- C나 gamma 값을 증가시켜 좀 더 복잡한 모델을 만들 수 있다.

> ### 장단점과 매개변수

- SVM은 데이터셋의 특성이 몇 개 안 되더라도 복잡한 결정 경계를 만들 수 있다
- 저차원과 고차원(특성이 적거나 많을 때)에 모두 잘 동작하지만 샘플이 많을 때는 잘 맞지 않음
- 데이터 전처리와 매개변수 설정에 신경을 많이 써야 함
- 분석하기 어려움
<br>
- 커널 SVM에서 중요한 매개변수는 규제 매개변수인 **C**
- RBF 커널은 가우시안 커널 폭의 역수인 **gamma** 매개변수를 가짐
- C와 gamma 모두 큰 값이 더 복잡한 모델을 만든다
<br>

# 신경망(딥러닝)

**다층 퍼셉트론**은 복잡한 알고리즘의 출발점이며 비교적 간단하게 분류와 회귀에 쓸 수 있다.
- 피드포워드 신경망 또는 그냥 신경망이라고도 함

> ### 신경망 모델

**MLP** *multi-layer perceptron* 는 여러 단계를 거쳐 결정을 만들어내는 선형 모델의 일반화된 모습이라고 볼 수 있다

<그림 2-44>의 **왼쪽 노드는 입력 특성**을 나타내며
**연결선은 학습된 계수**를 표현하고
**오른쪽 노드는 입력의 가중치 합, 즉 출력**을 나타낸다

- MLP에서 가중치 합을 만드는 과정이 여러 번 반복됨

- **계수**는 각 입력과 은닉층의 은닉 유닛 사이, 그리고 각 은닉 유닛과 출력 사이마다 있다

신경망 모델을 선형 모델보다 강력하게 만들기 위해 각 은닉 유닛의 가중치 합을 계산한 후 그 결과에 비선형 함수인 **렐루**나 **하이퍼볼릭 탄젠트**를 적용한다.
- ReLU는 0 이하를 잘라버리고, tanh 함수는 낮은 입력값에 대해서는 -1로 수렴하고 큰 입력값에 대해서는 +1로 수렴한다.

비선형 함수를 이용하여 신경망이 더 복잡한 함수 학습하게 됨
<br>

> ### 장단점과 매개변수

- 대량의 데이터에 내재된 정보를 잡아내고 매우 복잡한 모델을 만들 수 있다
- 크고 강력한 모델에서 종종 학습이 오래 걸린다
- 데이터 전처리 과정이 복잡하다
- 신경망의 중요 매개변수는 **은닉층의 개수**와 **각 은닉층의 유닛 수**
<br>

# 결정 함수

scikit-learn 분류기에서 불확실성을 추정할 수 있는 함수
- decision_function
- predict_proba()
<br>
- 이진 분류에서 decision_function의 반환값의 크기는
(n_samples,)이며 각 샘플이 하나의 실수 값을 반환한다
- decision_function 값의 범위는 데이터와 모델 파라미터에 따라 달라진다

> ### 예측 확률

- predict_proba() 출력은 각 클래스에 대한 확률
- 이 값의 크기는 이진 분류에서는 항상 (n_samples, 2)

- 첫 번째 원소 = 첫 번째 클래스의 예측 확률
두 번째 원소 = 두 번째 클래스의 예측 확률
- 확률이므로 항상 0과 1 사이의 값이며 합은 항상 1
