---
layout: post
title: 4주차 ML 지도학습 두번째 시간(2)
data: 2021-10-06 20:00:00 +0900
author: goldtan
categories: ["1st_term"]
tags: ["ml"]
---

# 2.3 지도 학습 알고리즘
## 2.3.8 커널 서포트 벡터 머신 (SVM)
<br>
서포트 벡터 머신은 회귀 문제에 사용되는 SVR과 분류 문제에 사용되는 SVC로 나눌 수 있는데,<br>
책에서는 SVC만을 서포트 벡터 머신이라고 표현하기 때문에 <br>
이번 포스트에서는 책과 동일하게 서포트 벡터 머신을 분류 모델로 고려하여 작성하였습니다.
<br>

---
<br>
<span style="color:#4CAF50"> 선형 모델과 비선형 특성</span><br>
<br>
<p>
책에서는 커널 서포트 벡터 머신에 대하여<br>
<b>"입력 데이터에서 단순한 초평면으로 정의되지 않는 더 복잡한 모델을 만들 수 있도록 확장한 것"</b> 이라고 <br>
정의하고 있습니다.<br>
하지만 이러한 표현은 너무 와닿지 않았기 때문에 제가 이해한 방법으로 SVM을 설명해보고자 합니다.
</p>
<br>
<p>
우선, 왜 우리가 SVM을 사용하는지부터 이야기를 해야 할 것 같습니다.<br>
SVM은 Logistic Regression과 같은 선형 분류 모델입니다.<br>
선형 분류기(Classifier)의 목표는 d차원에서 두 범주를 잘 구분하는 
(d-1)차원의 초평면(hyper palne)을 찾는 것입니다. 
</p>
<br>

![선형모델과비선형특성0](https://user-images.githubusercontent.com/83542989/136400127-97d727ab-fe13-48b8-9b2f-fa9c81d08e3b.png)

``` 
초평면(hyper plane) 
2차원에서의 범주를 구분하기 위한 선형적인 기준은 직선, 3차원에서는 평면(2차원)
하지만 3차원이 넘어갈 경우, 범주를 구분하기 위한 선형적인 기준을 표현하기 어렵기 때문에 
"초평면"이라는 표현을 사용
```
<p>
하지만 초평면은 유연하지 않기 때문에 선형으로 구분되지 않는 문제를 해결할 수 없다는 한계가 있습니다.<br>
이를 해결하기 위해 우리는 데이터에 비선형적인 특성을 추가하여 차원을 늘리는 방법을 사용합니다. <br>
그 이유는 고차원에서는 보다 쉽게 선형적인 평면을 통해 분류를 할 수 있기 때문입니다.<br> 고차원에서의 선형 분류 이후 다시 이전의 특성으로 투영하게 되면,<br> 결정 경계가 선형을 띄지 않고, 위의 그림처럼 비선형적인 모습을 띄는 것을 알 수 있습니다.<br> 
이것이 바로 비선형 특성의 이유입니다.

</p>
<br>

---
<br>
<span style="color:#4CAF50">커널 기법(Kernel trick)</span>
<br>
<br>
<p>
이제 선형과 비선형 모든 문제를 해결할 수 있을 것 같지만, 사실 앞에서 설명한 방법에는 문제가 하나 있습니다.<br>
저차원에서 선형 구분이 되지 않을 때, 더 높은 차원으로 대응시키면 분리를 쉽게 할 수 있는 것은 맞지만,<br> 어떠한 비선형적인 특성을 추가할 지 판단하기 어렵고, 무엇보다 그러한 과정에서 연산량이 크게 증가합니다.<br>
따라서 이를 해결하기 위해 우린 커널 기법을 사용합니다. 
</p>
<br>
<p>
커널 기법을 사용하게 되면 마치 비선형적인 특성 여러 개를 추가한 것과 같은 효과를 낼 수 있습니다.<br> 
그리고 이 과정 속에서 벡터 간 거리까지 구할 수 있기 때문에 상당히 효율적입니다.<br>
보통 가장 많이 사용하는 것은 default로 설정되어 있는 rbf 커널이고, 이는 가우시안 커널이라고도 합니다.<br>
아래의 그림은 왼쪽부터 각각 'linear', 'poly', 'rbf', 'sigmoid' 커널을 사용하여 분류한 결과입니다.
</p>
<br>

![커널기법](https://user-images.githubusercontent.com/83542989/136400165-7c553794-9b21-4866-9e59-3a6c6b0aa9e1.png)

<br>

---
<br>
<span style="color:#4CAF50">SVM 이해하기</span>
<br>
<br>
<p>
SVM이 결정 경계를 만들 때, 모든 데이터가 결정 경계에 영향을 주는 것은 아닙니다.<br> 
일부 외곽에 있는 데이터 포인트들만 결정 경계를 생성하는데 영향을 주게 되는데,<br> 이러한 (아래 그림에 테두리가 있는) 포인트들을 우리는 서포트 벡터라고 부릅니다.<br>서포트 벡터 머신이라는 이름이 붙여진 이유이기도 합니다.<br>
따라서 훈련 과정에서 서포트 벡터들을 이용하여 결정 경계를 형성하고,<br> 각 서포트 벡터마다 결정 경계에 영향을 끼치는 정도가 다르기 때문에 <br>이를 학습하는 방식으로 모델이 구성되어 있습니다.
</p>
<br>

![svm이해하기](https://user-images.githubusercontent.com/83542989/136400174-02890df5-6ce7-4f1f-bcf5-3b4bd4c9098e.png)

<br>
<p>
그럼 두 결정 경계가 모두 성공적으로 모든 데이터를 분류할 경우에는 어떠한 경계를 선택해야 할까요 ??<br>
SVM은 선형 분류기이기도 하지만 최대 마진 분류기(Maximal Margin Classifier) 입니다.<br>
따라서 SVM은 훈련을 통해 이 마진이 최대가 되도록 하는 결정 경계를 선택합니다.<br>
위의 그림에서는 파란색의 결정 경계가 가장 바람직합니다.
<br>
<br>

```
마진(Margin)
결정 경계와 가장 가까운 서포트 벡터 사이의 거리
```

</p>
<br>

---
<br>
<span style="color:#4CAF50">SVM 매개변수(parameter) 튜닝</span>
<br>
<br>

SVM은 매개변수 설정에 매우 민감한 모델입니다. 따라서 세밀한 조정이 필요합니다.<br>

* C<br>
C는 규제 매개변수로 선형 모델에서 사용한 것과 유사합니다.<br>
다만, 여기서는 훈련 과정에서 학습한 각 포인트의 중요도를 제한합니다.<br><br>
**C 값이 높아지면 규제 감소, C 값이 작아지면 규제 강화**

<br>

* gamma(γ) <br>
gamma(γ) 는 하나의 훈련 샘플이 미치는 영향 범위<br>
gamma(γ)는 SVM 자체의 매개변수는 아닙니다.<br>
하지만 default로 사용되는 rbf 커널의 매개변수이기 때문에 알아두는 편이 좋을 것 같습니다.<br><br>
**gamma(γ) 작으면 넓은 영역에 영향, gamma(γ) 크면 좁은 영역에 영향**

<br>

![svm매개변수튜닝](https://user-images.githubusercontent.com/83542989/136400171-50573459-98e1-48b1-bae5-8fbeb591f889.png)

<br>

---
<br>
<span style="color:#4CAF50">SVM을 위한 데이터 전처리(Data preprocessing)</span>
<br>
<br>
<p>
SVM은 매개변수 설정에도 민감하다고 했지만, 데이터 스케일에도 매우 민감합니다.<br> 
책에서 사용한 방법은 MinMaxScaler (최댓값은 1로, 최솟값은 0으로) (자세한 방법은 3장에서 ...)<br>
간단한(?) 전처리를 통해 성능이 눈에 띄게 향상하는 것을 볼 수 있습니다.
</p>
<br>

![svm데이터전처리](https://user-images.githubusercontent.com/83542989/136400169-749edbb4-2118-4855-b485-63bc383b84ec.png)

<br>

---
<br>
<span style="color:#4CAF50">SVM의 장단점</span>
<br>
<br>

* 장점<br>
강력하며, 다양한 데이터셋에 잘 작동한다.<br>
특성이 몇 개 되지 않더라도 복잡한 결정 경계 만들 수 있다.


<br>

* 단점<br>
샘플이 많을 때는 잘 맞지 않는다. ( 속도와 메모리 관점 )<br>
데이터 전처리와 매개변수 설정이 중요하다.

<br>

**모든 특성이 비슷한 단위이고 스케일이 비슷할 때, SVM을 시도해보자 !**

<br>

---

---

<br>

## 2.3.9 신경망 (딥러닝)
<br>

---
<br>
<span style="color:#FFC107">신경망 모델</span><br>
<br>
<p>
인공신경망(Artificial Neural Network)은 동물의 뇌에서 영감을 얻은 알고리즘입니다.<br>
ANN에서의 퍼셉트론은 마치 뇌의 뉴런과 같은 역할을 한다고 합니다.<br>
사실 고등학교 때 과학을 배우지 않아서 이에 대해 잘 모르지만, <br>
혹시 과학을 배우신 분들이라면 이것이 이해를 도울 수 있을까 싶어 이렇게 적어두었습니다.
</p>
<br>

![신경망모델](https://user-images.githubusercontent.com/83542989/136400141-5777f86a-b556-48a5-8623-75cd3bdc85cf.png)

<br>
<p>
이 퍼셉트론을 여러 층 쌓아서 구성한 네트워크가 다층 퍼셉트론(Multi-Layer Perceptron)입니다.<br> 
MLP의 특징은 아래와 같이 각각 입력층과 출력층이 있고, 이 사이에 여러 층의 은닉층(hidden layer)을 넣을 수 있습니다.<br>
이렇게 여러 개의 은닉층을 통해 구성된 네트워크를 심층 신경망(Deep Neural Network)라고 부르며, <br>
이를 학습하기 위한 알고리즘을 딥러닝(Deep Learning)이라고 부릅니다.
</p>
<br>

![신경망모델1](https://user-images.githubusercontent.com/83542989/136400144-cc93592c-8e51-4d18-acaa-fb92428c951e.png)

<br>
<p>
하지만 선형 함수는 여러 층 쌓더라도 선형의 형태가 되기 때문에 비선형 문제를 해결할 수 없습니다.<br>
따라서 우리는 비선형 문제를 해결하기 위해 비선형 함수인 활성화 함수(activation function)를 사용합니다.<br>
렐루(Relu), 하이퍼볼릭 탄젠트(tanh) 이외에도 Sigmoid, Step function 등이 있습니다.<br>
각 함수의 형태는 왼쪽 아래의 그래프를 참고하시면 됩니다.
</p>
<br>

![신경망모델2](https://user-images.githubusercontent.com/83542989/136400145-44164b1b-7a14-42eb-8ba5-34de1093a1ae.png)

<br>

---
<br>
<span style="color:#FFC107">신경망 튜닝</span>
<br>
<br>
<p>
신경망은 조절할 수 있는 매개변수들이 많습니다.<br>
그 중 (한 층에 있는) 은닉 유닛의 개수와 은닉층의 개수는 늘어날수록, 모델의 복잡도는 높아집니다.<br>
다양한 활성화 함수를 통해서도, 각기 다른 결정 경계를 얻을 수 있습니다.
</p>
<br>

![신경망튜닝1](https://user-images.githubusercontent.com/83542989/136402696-3d5c2076-17a0-4ac5-b4df-42f0ff98f942.png)

<br>
<p>
이외에도 L2 패널티를 사용하여 모델 복잡도를 제어할 수 있습니다.<br>
이는 사용하는 모델마다 다르지만 책에 나온 MLPClassifier에서는 alpha가 이 역할을 수행합니다.<br>
규제(alpha)가 클수록 모델의 복잡도는 낮아집니다.
<br>
<br>

![신경망튜닝2](https://user-images.githubusercontent.com/83542989/136400155-7db57d75-4071-4fcc-9976-12551f7af005.png)

<br>

```
cf. dropout
인공신경망에서 과적합을 줄이기 위한 정규화 기술입니다. 
신경망의 훈련과정에서 무작위로 은닉 유닛을 "제거" 혹은 "생략"시켜 훈련 데이터를 완전하게 학습하지 못하도록 하여 과적합을 막습니다.
```

![신경망튜닝](https://user-images.githubusercontent.com/83542989/136400148-dd9e1685-9e82-4b67-a8ff-9b2c03701e4e.png)

</p>
<br>

---
<br>
<span style="color:#FFC107">복잡도 추정</span>
<br>
<br>
<p>
보통 신경망의 복잡도는 가중치의 수(계수의 수)를 의미합니다.<br>
첫번째 층과 두번째 층 사이의 가중치를 계산하는 방법은<br>
첫번째 층의 output이 두번째 층의 input이 되기 때문에, 첫번째 층의 output의 수와 두번째 층의 유닛 수를 곱하고,<br> 
두번째 층의 각 유닛마다 bias가 존재하기 때문에, 두번째 층의 유닛 수를 다시 더하면 계산할 수 있습니다.
</p>
<br>

![복잡도추정](https://user-images.githubusercontent.com/83542989/136400119-eb408cda-f0ce-409f-93fa-7c67a3d15559.png)

<br>

---
<br>
<span style="color:#FFC107">신경망의 장단점</span>
<br>
<br>

* 장점<br>
대량의 데이터에 내재된(인간이 찾지 못하는) 정보를 잡아내고 매우 복잡한 모델을 만들 수 있다.
<br>
충분한 시간과 데이터를 주고 매개변수를 세심하게 조정하면 훌륭한 성능을 낸다.



<br>

* 단점<br>
학습이 오래 걸리고 전처리에 주의해야 한다.
<br>
책에서는, "신경망 매개변수 튜닝은 예술에 가까운 일" 이라고 표현한다....




<br>

---
---
---

<br>

# 2.4 분류 예측의 불확실성 추정
분류 예측의 불확실성 추정은 우리가 한 예측이 어느 정도의 정확성을 갖고 있는지 추정하는 것입니다.<br>
특정 데이터를 분류하였을 때, 분류한 결과에 확신을 갖는 정도 혹은 분류한 결과가 맞을 확률 등으로 표현할 수 있습니다.
## 2.4.1 결정 함수
<br>
<p>
결정 함수를 통해 출력된 값은 데이터 포인트가 특정 클래스에 속한다고 믿는 정도입니다.<br>
따라서 이진 분류에서 decision_function의 반환값은 (n_samples,) 과 같습니다.<br>
출력된 값의 절댓값이 클수록 분류한 클래스에 속한다고 믿는 정도가 크다고 할 수 있습니다.<br>
그런데 출력값의 절댓값이 어떻게 믿는 정도가 될 수 있는지 의문이 들 수 있습니다.<br>
</p>
<br>

![결정함수1](https://user-images.githubusercontent.com/83542989/136400179-f9a3784d-c9c7-4ec9-b656-0041b2946fc2.png)

<br>

![결정함수](https://user-images.githubusercontent.com/83542989/136400177-ab6cbb0d-3a9f-4f97-9454-96eed07bb913.png)

<br>
<p>
위의 예시를 보시면, '5' 와 '5가 아닌 것'으로 분류한다고 가정하였을 때, <br>
양극단의 데이터들은 확실하게 '5' 혹은 '5가 아닌 것'으로 분류할 수 있지만 그 사이에는 애매한 데이터들도 있습니다.<br>
분류에서는 결정 함수의 출력값을 순서대로 나열한 후에 임계값(threshold)을 통해 분류하게 되는데,<br> 
따라서 이진 분류에서는 0이 임계값이 되고 보통 부호를 기준으로 분류하게 됩니다.<br>
하지만 만약 임계치를 높이거나 낮춘다면, 절댓값이 낮은 중앙에 위치한 데이터들은 분류 결과가 달라질 것입니다.<br>
하지만 절대값이 높은 친구들은 쉽게 변하지 않을 것입니다. <br>
따라서 우리는 이 decision_function의 출력값을 특정 클래스에 속한다고 믿는 정도라고 말할 수 있습니다.
</p>
<br>

![결정함수2](https://user-images.githubusercontent.com/83542989/136400103-a0733589-f52a-41e9-9dce-d4254af40172.png)

<br>

---
---
<br>

## 2.4.2 예측 확률
<br>
<p>
예측 확률을 통해 출력된 값은 데이터 포인트가 특정 클래스에 속할 확률입니다.<br>
따라서 이진 분류에서 predict_proba의 반환값은 (n_samples, 2) 와 같습니다.<br>
각 행의 첫 번째 원소는 첫 번째 클래스일 확률, 두 번째 원소는 두 번째 클래스일 확률이며, 합은 항상 1 입니다.<br>
</p>
<br>

![예측확률1](https://user-images.githubusercontent.com/83542989/136400163-c230fcf5-2d5e-448b-af0c-5cf6b836638c.png)

<br>
<p>
일반적으로 복잡도가 낮은 모델이 예측에 불확실성이 더 높다고 하며,<br>
불확실성과 모델의 정확도가 동등하면 모델이 보정(calibration)되었다고 합니다.
</p>
<br>

![예측확률](https://user-images.githubusercontent.com/83542989/136400161-f09722f7-f6d3-4967-91da-e40726fd2b4a.png)

<br>

---
---
<br>

## 2.4.3 다중 분류에서의 불확실성
<br>
<p>
decision_function 과 predict_proba 메서드는 다중 분류에도 사용이 가능합니다.<br>
다만, 출력은 (n_samples, n_classes)의 형태라는 점을 기억해야 합니다.<br>
그리고 또 한 가지 주의할 점은 메서드의 argmax를 적용한 값은 index의 형태로 출력이 되기 때문에, <br>
index가 아닌 별도의 클래스 이름이 있을 경우 index와 클래스 이름을 연결시켜줘야 합니다.
</p>
<br>

![다중분류불확실](https://user-images.githubusercontent.com/83542989/136400113-777b2316-e940-448e-8d79-707ab1f5a996.png)

<br>

---
---
---
<br>

# 마무리

발표 환경도 낯설고 준비한 대본도 읽기가 어려워 긴장이 많이 되었던 것 같습니다. <br>
매주 발표해주시는 코어분들과 멤버분들에게 다시 한 번 감사함을 느끼게 되었고<br>
다음 발표 때는 대본 없이도 더 잘 할 수 있었으면 좋겠습니다 ㅎㅎㅎ ....<br>
대신 포스트를 발표한다고 생각하고 열심히 작성하였기 때문에 <br>
혹시 발표 내용이 부족하셨던 분들에게 이 포스트가 도움이 되었으면 좋겠습니다.<br>
### GDSC 화이팅 ! ML 화이팅 !