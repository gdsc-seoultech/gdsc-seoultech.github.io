---
layout: post
title: 4주차 ML 지도학습 WIL
date: 2021-10-02 20:00:00 +0900
author: seongryool
description:
categories: ["1st_term"]
tags: ["ml"]
---

# 결정트리

- 결정 트리를 학습한다는 것은 정답에 가장 빨리 도달하는 예/아니오 질문 목록을 학습한다는 뜻

- 트리를 만들 때 알고리즘은 가능한 모든 테스트에서 타깃 값에 대해 가장 많은 정보를 가진 것을 고른다.(정보이득, 지니계수, 균일도)

- 데이터를 분할하는 것은 각 분할된 영역이 한 개의 target 값을 가질 때까지 반복된다.
  target 하나로만 이뤄진 leaf node를 pure node라고 한다.

- 새로운 데이터 포인트에 대한 예측은 주어진 데이터 포인트가 특성을 분할한 영역들 중 어디에 놓이는지를 확인하면 된다.

### 결정트리 복잡도 제어

- 보통 tree를 만들 때 모든 leaf 노드가 순수 노드가 될 때까지 진행하면 model이 매우 복잡해지고 훈련 데이터에 overfitting 된다. 즉 pure node로 이뤄진 tree는 훈련 세트에 100% 정확하게 맞는다는 의미이다.

- overfitting을 막는 전략은 크게 두 가지이다.

  ### 사전 가지치기

  tree 생성을 일찍 중단하는 사전 가지치기 방법  
   사전 가지치기는 트리의 최대 깊이나 leat의 최대 개수를 제한하거나 또는 노드가 분할하기 위한 포인트의 최소 개수를 지정하는 것이다.

- max_depth : 일정 깊이에 도달하면 트리의 성장을 멈춘다. 깊이를 제한하면 overfitting이 줄어들어 훈련 세트의 정확도를 떨어뜨리지만 테스트 세트의 성능은 개선시킨다.
- max_leaf_nodes :리프 노드의 최대 개수를 지정하는 매개변수
- min_samples_leaf는 리프 노드가 되기 위한 최소한의 샘플 개수

### 사후 가지치기

tree를 만든 후 데이터 포인트가 적은 노드를 삭제하거나 병합하는 전략

### 트리의 특성 중요도

- feature importance를 사용하면 트리를 만든 결정에 각 특성이 얼마나 중요한게 사용되었는지를 알 수 있다.
- 이 값은 0과 1 사이의 숫자로, 각 특성에 대해 0은 전혀 사용되지 않았고 1은 완벽하게 타깃 클래스를 예측.
- feature importance의 값이 낮다고 해서 유용하지 않다는 뜻은 아니고 단지 트리가 그 특성을 선택하지 않았거나 다른 특성이 동일한 정보를 지니고 있을 수도 있기 때문이다.

- ### tree 모델 한계, 장단점

  - tree 모델의 regressor는 모델이 가진 데이터 범위 밖으로 나가면 단순히 마지막 포인트를 이용해 예측을 한다. 즉 트리 모델은 훈련 데이터 밖의 새로운 데이터를 예측할 능력이 없다.

  - 결정 트리는 모델을 쉽게 시각화할 수 있기에 이해하기 쉽고 데이터의 스케일에 구애 받지 않는다.

# 랜덤 포레스트

- 앙상블이란 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법

- 랜덤 포레스트는 조금씩 다른 여러 결정 트리의 묶음이다. 즉 여러 트리 모델을 만들고 그 결과를 평균내어서 사용한다.

- 랜덤 포레스트에서 tree를 랜덤하게 만드는 방법은 두 가지. 1. 데이터 포인트를 무작위로 선택 2. 분할 테스트에서 특성을 무작위로 선택

- 부트스트랩 샘플이용 : n_samples개의 데이터 포인트중에서 무작위로 데이터를 n_samples 횟수만큼 반복 추출한다.

### 랜덤 포레스트 장단점

- 단일 트리의 단점을 보완하고 장점은 그대로 가지고 잇다.
- 수 백개의 트리를 자세히 분석하기는 어렵고 랜덤 포레스트의 트리는 결정 트리보다 더 깊어지는 경향도 있다.
- 텍스트 데이터 같이 매우 차원이 높고 희소한 데이터에는 잘 작동하지 않는다.
- 선형 모델보다 많은 메모리를 사용하며 훈련과 예측이 느리다.

# 그레이디언트 부스팅 회귀 트리

- 여러 개의 결정 트리를 묶어 모델을 만드는 앙상블 방법, 이름이 회귀지만 회귀와 분류에 모두 사용 가능
- 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만든다. 이게 부스팅 방식
- 무작성이 없고 강력한 사전 가지치기 사용
- learning_rate : 이전 트리의 오차를 얼마나 강하게 보정할 것인지 제어
- 단점 : 매겨변수를 잘 조정해야하고 훈련 시간이 길다.
- 장점 : 다른 트리 기반 모델처럼 특성의 스케일을 조정하지 않아도 되고 이진 특성이나 연속적인 특성에서도 잘 동작한다.
- 모델의 특성상 희소한 고차원 데이터에는 잘 작동하지 않는다.
- n_estimators를 크게 하면 모델이 복잡해지고 과대적합될 가능성이 높아진다.
- 보통 max_depth를 매우 작게 설정하고 트리의 깊이가 5보다 깊어지지 않게 한다.

# 커널 서포트 벡터 머신 SVM

- SVM은 입력 데이터에서 단순한 초평면으로 정의되지 않는 더 복잡한 모델을 만들 수 있도록 확장한 것
- 선형 모델을 유연하게 만들려면 특성끼리 곱하거나 특성을 거듭제곱하는 식으로 새로운 특성을 추가.
- 커널 기법 : 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리를 계산한다.

  - 원래의 특성의 가능한 조합을 지정된 차수까지 계산
  - 가우시안 커널로 불리는 RBF 커널 : 차원이 무한한 특성 공간에 매핑하는 것으로 모든 차수의 모든 다항식을 고려한다고 생각하면 됨. 특성의 중요도는 고차항이 될수록 줄어든다.

- 서포트 벡터 : 두 클래스 사이의 경계에 위치한 데이터 포인트
  - 새로운 데이터 포인트에 대해 예측하려면 서포트 벡터와의 거리 측정
- gamma : 가우시안 커널 폭의 역수, 하나의 훈련 샘플이 미치는 영향의 범위 결정

  - 작은 값은 넓은 영역, 큰 값은 좁은 영역
  - 가우시안 커널의 반경이 클 수록 훈련 샘플의 영향 범위도 커짐

- C : 규제 매개변수, 각 포인트의 중요도를 제한한다.
- SVM에서의 데이터 전처리는 특성 값을 평균 0, 단위 분산 or 0과 1 사이로 맞추는 방법 많이 사용
- 다양한 데이터셋에서 잘 작동, 데이터의 특성이 몇 개 없어도 복잡한 결정 경계 만들 수 있다.
- 특성이 적을 때와 많을 때에 모두 잘 작동하지만 샘플이 많을 때는 잘 맞지 않는다.
- 단점으론 데이터 전처리와 매개변수 설정에 신경을 많이 써야한다.
- 모든 특성이 비슷한 단위고 스케일이 비슷하면 SVM을 시도할만 하다.

# 신경망

- 다층 퍼셉트론(MultiLayer Perpeptros,MLP)
- 가중치 합을 만드는 과정이 여러 번 반복되고, 먼저 중간 단계를 구성하는 hidden unity 계산 후 이를 이용하여 최종 결과를 산출하기 위해 다시 가중치 합을 계산.
- 각 은닉 유닛의 가중치 합을 계산한 후 그 결과에 비선형 함수인 ReLU, tanh를 적용
- 은닉층으로 구성된 대규모의 신경망이 생기면서 딥러닝이라고 부름
- alpha : 패널티를 의미, 모델의 복잡도 제어, alpha 작으면 가중치 커짐
- 가중치를 무작위로 설정하는 것이 모델 학습에 영향을 끼침
- MLP 또한 데이터 스케일에 영향을 많이 받음
- 대량의 데이터에 내재된 정보를 잡아내고 매우 복잡한 모델을 만들 수 있다는 점
- 학습 시간이 오래 걸리고 데이터 전처리에 주의
- SVM과 비슷하게 모든 특성이 같은 의미를 가진 데이터에서 잘 작동
- 다른 종류의 특성을 가진 데이터면 트리 기반의 모델이 잘 작동할 수 있다.

# 정리

- KNN : 작은 데이터셋일 경우, 기본 모델로 좋고 설명하기 쉽다.
- 선형 모델 : 대용량 데이터셋 가능, 고차원 데이터에 가능
- 나이브 베이즈 : 분류만 가능, 데용량 데이텃과 고차원 데이터 가능
- 결정 트리 : 매우 빠름, 스케일 조정 필요 없다.
- 랜덤 포레스트 : 안정적이고 강력함. 데이터 스케일 조정 필요 없음, 고차원 희소 데이터에는 잘 안 맞음
- 그레이디언트 부스터 결정트리 : 랜포보다 좀 더 성능이 좋음, 학습은 느리나 예측은 빠르고 메모리를 조금 사용, 매개변수 튜닝 많이 필요
- 서포트 벡터 머신 : 비슷한 의미의 특성으로 이뤄진 중간 규모 데이터셋에 잘 맞음. 데이터 스케일 조정 필요, 매개변수에 민감
- 신경망 : 대용량 데이터셋에서 매우 복잡한 모델을 만들 수 있음. 매개변수 선택과 데이터 스케일에 민감. 큰 모델은 학습이 오래 걸림
