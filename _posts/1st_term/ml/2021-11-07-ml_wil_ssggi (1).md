---
layout: post
title: 6주차 ML 비지도학습 WIL 
date: 2021-11-01
author: ssggi
categories: ["1st_term"]
tags: ["ml"]
---

- 군집 :
데이터를 클러스터라는 그룹으로 나눔
그룹 안의 데이터 포인트는 매우 비슷, 다른 클러스터의 데이터 포인트와는 구분되도록 나누는 것이 목표

- 군집 알고리즘 : 
데이터 포인트가 어느 클러스터에 속하는지 할당 또는 예측

# k-평균 군집
```python
from sklearn.cluster import KMeans
```

가장 널리 사용하는 군집 알고리즘
클러스터 중심을 찾는다.
1단계. 데이터 포인트를 가장 가까운 클러스터 중심에 할당
2단계. 클러스터에 할당된 데이터 포인트의 평균으로 클러스터 중심을 다시 지정
클러스터에 할당되는 데이터 포인트에 변화가 없을 때 알고리즘 종료

- 사이킷런에서 k-평균 알고리즘 사용 ->
```python
from sklearn.datasets import KMeans
```
- 인위적인 2차원 데이터 생성 -> 
```python
from sklearn.datasets import make_blobs
```
- X, y = make_blobs(random_state=???) 
- 클러스터 수 지정 -> KMeans(n_clusters = ) (지정하지 않으면 기본값 8)
- fit 메서드 사용 -> kmeans.fit(X)

- predict 메서드 사용 -> kmeans.predict(X) : 새로운 데이터의 클러스터 레이블 예측 가능
- 훈련세트에 대해 predict 메서드 사용 -> kmeans.labels_

분류와 비슷한 점: 각 데이터 포인트가 레이블을 가진다
다른 점: 정답을 모르며 레이블 자체에 의미가 없다.

k-평균 알고리즘이 실패하는 경우
클러스터가 둥근 형태로 나타나므로 복잡한 모양의 클러스터를 구분하지 못함

- PCA : 데이터에서 분산이 가장 큰 방향을 찾으려고 함
```python
from sklearn.decomposition import PCA
```
- NMF : 데이터의 극단 또는 일부분에 상응되는 중첩할 수 있는 성분을 찾음
```python
from sklearn.decomposition import NMF
```
- 벡터 양자화 : k-평균을 각 포인터가 하나의 성분(클러스터 중심)으로 분해되는 관점으로 보는 것


> ### k- 평균 단점
무작위 초기화를 사용하여 알고리즘의 출력이 난수 초깃값에 따라 달라짐
활용 범위가 비교적 제한적(복잡한 클러스터 구분 잘 못함)이고 클러스터 개수를 지정해주어야 함

# 병합 군집
알고리즘을 시작할 때 각 포인트를 하나의 클러스터로 지정 (데이터 포인트 수 = 클러스터 수)
어떤 종료 조건을 만족할 때까지 가장 비슷한 두 클러스터를 합쳐 나감
새로운 테스트 데이터에 대해 예측 불가

scikitlearn에서 종료 조건 : 클러스터 개수. 지정된 클러스터 개수가 남을 때까지 클러스터를 합침 (240 그림 참고)

사이킷런에 구현된 옵션 ward, average, complete (239)

### 계층적 군집과 덴드로그램
병합 군집은 계층적 군집을 만든다 등고선 느낌의 그림?
특성이 셋 이상인 데이터셋에 불가

덴드로그램은 다차원 데이터셋 처리 가능
```python
from scipy.cluster.hierarchy import dendrogram, ward
X, y = make_blobs(random_state=0, n_samples=12)
linkage_array = ward(X)
dendrogram(linkage_array)
```
덴드로그램 가지의 길이 : 합쳐진 클러스터가 얼마나 떨어져 있는지를 보여줌

# DBSCAN
클러스터의 개수를 미리 지정할 필요가 없는 군집 알고리즘
복잡한 형상 찾을 수 o. 
어떤 클래스에도 속하지 않는 포인트를 구분할 수 있다. (잡음)
k-평균이나 병합 군집보다 느리지만 큰 데이터셋에 적용 가능

데이터가 많아 붐비는 “밀집 지역”을 찾는다.
밀집 지역이 한 클러스터를 구성, 비교적 비어있는 지역을 경계로 다른 클러스터와 구분됨.

밀집 지역에 있는 포인트를 “핵심 샘플”이라고 함

매개변수 -> min_samples, eps
eps 거리 안에 데이터가 min_samples 개수만큼 들어 있는 데이터 포인트를 “핵심 샘플”로 분류
eps보다 가까운 핵심 샘플은 DBSCAN의 동일한 클러스터로 합쳐짐

1. 시작할 때 무작위로 포인트를 선택
2. 그 포인트에서 eps 거리 안의 모든 포인트를 찾음
3. eps 거리 안에 데이터가 min_samples 개수보다 적다면 “잡음”(어떤 클래스에도 속하지 않음)으로 레이블
4. eps 거리 안에 데이터가 min_samples 개수보다 많다면 핵심 샘플로 레이블, 새로운 클러스터 레이블을 할당
5. 새로운 클러스터 레이블의 eps 거리 안의 모든 이웃을 살피고 핵심 샘플이면 그 포인트의 이웃을 차례로 방문
6. 반복
7. 클러스터는 eps 거리 안에 더 이상 핵심 샘플이 없을 때까지 자라남

포인트의 종류 : 핵심 포인트, 경계 포인트(핵심 포인트에서 eps 거리 안에 있는 포인트), 잡음 포인트(-1로 레이블됨)
(경계 포인트는 포인트를 방문하는 순서에 따라 달라지나 중요한 이슈는 아님)

DBSCAN을 한 데이터셋에 여러 번 실행 -> 핵심 포인트의 군집은 항상 같고 매번 같은 포인트를 잡음으로 레이블함.

- 새로운 데이터에 대해 예측 불가능
clusters = dbscan.fit_predict(X) : predict 메서드를 사용하여 군집과 클러스터 레이블을 한 번에 계산

>- eps 증가 -> 하나의 클러스터에 더 많은 포인트가 포함된다. 클러스터를 커지게 하지만 여러 클러스터를 하나로 합치게도 만든다.
- eps를 매우 작게 하면 -> 어떤 포인트도 핵심 포인트가 되지 못하고 모든 포인트가 잡음 포인트가 될 수도 있다.
- eps를 매우 크게 하면 -> 모든 포인트가 단 하나의 클러스터에 속하게 됨

>- min_samples를 키우면 핵심 포인트 수가 줄어들고 잡음 포인트가 늘어난다.
- min_samples를 늘리면 min_samples의 수보다 작은 클러스터들은 잡음 포인트가 된다.
-> 즉, min_samples는 클러스터의 최소 크기를 결정한다.

DBSCAN은 클러스터의 개수를 지정할 필요x, eps값이 간접적으로 몇 개의 클러스터가 만들어질지 제어한다.
적절한 eps값을 찾으려면 -> scaler = StandardScaler() 또는 MinMaxScaler()를 통해 모든 특성의 스케일을 비슷한 범위로 조정

## 군집 알고리즘을 적용하기 어려운 점
알고리즘이 잘 작동하는지 평가하거나 여러 알고리즘의 출력을 비교하기가 어렵다

> ### 타깃 값으로 군집 평가

- ARI와 NMI 

1(최적)과 0(무작위) 사이의 값을 제공. 군집 알고리즘 결과와 실제 정답 클러스터와 비교 평가 지표
(ARI는 음수가 될 수도 있다)

ARI 사용
```python
from sklearn.metrics.cluster import adjusted_rand_score(cluster1, cluster2)
```

NMI 사용
```python
from sklearn.metrics,cluster import normalized_mutual_info_score(cluster1, cluster2)
```
만약 ARI나 NMI가 아니라 import accuracy_score를 했다면 -> 할당된 클러스터 레이블과 실제 클러스터 레이블 이름을 비교 -> 의미없음

> ### 타깃 값 없이 군집 평가

타깃 값으로 군집을 평가하는 방법의 단점 : 보통 결과와 비교할 타깃이 존재하지 않음

- 실루엣 계수 

타깃 값이 필요 없는 군집용 지표. 실제로 잘 동작하지는 않음.
실루엣 점수는 클러스터의 밀집 정도를 계산. 높을수록 좋으며 최대 점수는 1이다.
밀집된 클러스터가 좋긴 하나 모양이 복잡할 때는 밀집도를 활용한 평가가 잘 들어맞지 않음

실루엣 점수 사용

```python
from sklearn.metrics.cluster import silhouette_score
```

실루엣 점수는 견고성 기반 지표라 실루엣 점수가 높다고 하더라도 신뢰할만하지 않다
클러스터를 직접 확인하는 방법뿐!

## 얼굴 데이터셋으로 군집 알고리즘 비교

1. DBSCAN
모든 데이터가 –1(잡음)으로 레이블 됨(하나의 큰 클러스터 외에 만들 수 없음) 
-> eps 값을 늘려 각 포인트의 이웃의 수 늘리고 min_samples 값을 낮춰 클러스터에 모을 포인트 수를 줄일 수 있음
** 이상치 검출 : 특이한 특징을 찾아내는 것

2. k-평균
비슷한 크기의 클러스터들을 만들 수 있지만 클러스터 개수 지정해야 됨
k-평균의 클러스터 중심을 원본 공간으로 돌리려면 
-> pca.inverse_transform(center).reshape(image_shape)
잡음 포인트의 개념이 없어 모든 포인트를 구별하기 때문에 중심에서 먼 포인트들은 중심과 많이 다르고 특별한 규칙이 없다.
(클러스터 수를 늘리는 방법이 있음)

3. 병합 군집
```python
agglomerative = AgglomerativeClustering(n_clusters=수)
```
# 군집 알고리즘 요약
k-평균과 병합 군집은 클러스터 개수 지정
DBSCAN은 eps 매개변수를 이용하여 클러스터 크기를 간접 조정 가능

k-평균, 병합, DBSCAN 모두 대량의 데이터셋에 사용 가능, 여러개의 클러스터로 군집을 만들 수 있다

각 장점 
1. k-평균
클러스터 중심을 사용해 클러스터를 구분, 각 데이터 포인트를 클러스터의 중심으로 대표할 수 있기 때문에 분해 방법이라고도 볼 수 있음
2. DBSCAN
클러스터에 할당되지 않는 잡음 포인트 인식 가능, 클러스터의 개수를 자동 결정
복잡한 모양을 인식할 수 있음
크기가 많이 다른 클러스터를 만들어 냄(장점이 될 수도, 단점이 될 수도)
3. 병합
전제 데이터읩 분할 계층도를 만들어주며 덴드로그램을 사용해 손쉽게 확인 가능

