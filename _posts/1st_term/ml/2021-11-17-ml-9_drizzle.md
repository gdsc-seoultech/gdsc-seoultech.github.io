---
layout: post
title: 8주차 ML 모델 평가와 성능 향상 강의자료
date: 2021-11-17
author: drizzle0171
categories: ["1st_term"]
tags: ["ml"]
---


# 지난 주 과제
지난 주에는
- kaggle (https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python ) 필사
- WIL

의 두 가지 과제가 있었습니다! 타이타닉에서 벗어날 수 없는 우리 ML...


<br>

이번 주에는 Chapter5의 **모델 평가와 성능 향상**을 다루어볼 예정입니다.

지금까지 우리는 지도 학습, 비지도 학습 이론과 특성 공학에 대해 배웠습니다. 비지도 학습을 평가하는 일은 매-우 정성적인 작업이므로, 이 파트에서는 지도 학습(회귀, 분류)에 집중해보겠습니다.

지금까지 우리는 지도 학습 모델을 평가하기 위해,
- train_test_split 함수를 사용하여 데이터셋을 훈련 세트와 테스트 세트로 나누고,
- 모델을 만들기 위해 훈련 세트에 fit 메서드를 적용하고,
- 모델을 평가하기 위해 테스트 세트에 score 메서드를 사용했습니다!

이 파트에서는 이러한 평가 방법을 조금 더 확장해볼 예정입니다. 

<br>
<br>

# 1. 교차검증
> 데이터를 여러 번 반복해서 나누고 여러 모델을 학습하는 방법

가장 널리 쓰이는 교차 검증은 k-겹 교차 검증입니다. 
보통 **5**, **10**을 k로 사용합니다. 만일 5-겹 교차 검증이라면, 데이터를 **폴드(Fold)** 라고 하는 부분 집합 5개로 나누게 됩니다. 그 후 모델을 선정하여 첫 번째 폴드를 테스트 세트로 사용하고 나머지 폴드를 훈련 세트로 사용하여 학습합니다. 그 후 두 번째 폴드는 두 번째 모델의 테스트 세트로 사용되게 됩니다. 이런 식으로 모델의 학습과 테스트가 반복됩니다. 

![](https://images.velog.io/images/drizzle0171/post/25bedd55-de0d-4908-bdbc-9f7c2d3562af/image.png)

## 1. scikit-learn의 교차 검증
```python
from sklearn.model_selection import cross_val_score
```
scikit-learn에서의 교차검증은 위와 같이 model_selection에 cross_cal_score 함수로 구현되어 있습니다.

cross_cal_score의 매개변수는 **평가하려는 모델, 훈련 데이터, 타깃 레이블 ** 입니다.

```python
iris = load_iris()
logreg = LogisticRegression(max_iter = 1000)
scores = cross_val_score(logreg, iris.data, iris.target)
```
scikit-learn 0.22 버전부터 3-겹 교차 검증에서 5-겹 교차 검증으로 바뀌었습니다. 폴드의 개수는 **cv 매개변수**를 사용해서 바꿀 수 있습니다. (최소 5-겹 교차 검증을 사용하는 것이 좋습니다!)

교차 검증에는 **cross_validate** 함수를 사용할 수도 있습니다. 이 함수는 cross_val_score 함수와 인터페이스가 비슷하지만, 분할마다 훈련과 테스트에 걸린 시간을 담은 딕셔너리를 반환합니다. 

## 2. 교차 검증의 장점
- 교차 검증은 훈련 세트와 테스트 세트가 여러 개가 나오게 됩니다. 이는 세트들의 임의성을 높혀줄 수 있고, 결국 테스트 세트의 정확도에 영향을 미치는 정도가 적어질 수 있을 것이라고 볼 수 있습니다.

- 또, 데이터를 여러 개로 나눔으로써 모델이 훈련 데이터에 얼마나 민감한지 알 수 있습니다. 이는 최악의 경우와 최선의 경우를 짐작할 수 있게 한다는 장점이 있습니다.
- 마지막으로 분할을 한 번 했을 때보다 데이터를 더 효과적으로 사용할 수 있다는 점입니다. train_test_split을 사용하면 데이터 중 75%를 훈련 세트에 사용하고 25%를 평가에 사용합니다. 하지만 k-겹 교차 검증을 사용하면 (k-1)/k를 모델 학습에 사용하므로 더 정확한 모델을 만들어낼 수 있습니다.
- 하지만 교차 검증 역시 한 가지 단점이 있습니다. 바로 **연산 비용이 늘어난다는 점**입니다. 모델을 k개나 만들어야 하므로 데이터를 한 번 나눴을 때보다 대략 k배 더 느립니다.

## 3. 계층별 k-겹 교차 검증과 그 외 전략들
k-겹 교차 검증은 데이터셋을 나열 순서대로 k개의 폴드로 나누게 됩니다. iris와 같이 첫 번째 1/3은 클래스 0, 두 번째 1/3은 클래스 1 과 은 데이터를 3-겹 교차 검증을 한다고 생각해보겠습니다. 그럼 첫 번째 폴드의 테스트 세트는 클래스 0, 훈련 세트는 클래스 1,2가 됩니다. 즉, 이 정확도는 '0'이 되겠죠?

이러한 문제를 해결하기 위한 방법이 바로 **계층별 k-겹 교차 검증**입니다. 아래의 사진과 같이 폴드 안의 클래스 비율이 전체 데이터 셋의 클래스 비율과 동일하도록 데이터를 나눕니다.

![](https://images.velog.io/images/drizzle0171/post/6c17eab8-93ed-4335-b41b-8684f4256e20/image.png)

분류에서는 이러한 계층별 K-겹 교차 검증을 많이 사용하기는 합니다만, 회귀에서는 기본 K-겹 교차 검증을 사용합니다. 물론 회귀에서도 계층별 k-겹 교차 검증을 사용할 수는 있지만, 일반적인 방법은 아닙니다.

<br>
1. 교차 검증 상세 옵션

scikit-learn에서는 cv 매개변수에 **교차 검증 분할기**를 전달하여 데이터를 분할할 때 더 세밀하게 제어할 수 있습니다. 대부분의 경우, 기본값이 잘 작동하지만, 가끔! 전략이 필요할 수 있습니다.

예를 들면, 다른 사람의 결과를 재현하기 위해 분류 데이터셋에 기본 k-겹 교차 검증을 사용해야 할 때입니다. 이렇게 하려면 model_selection 모듈에서 KFold 분할기를 import하고 원하는 폴드 수를 넣어 객체를 생성해야 합니다.

```python
from sklearn.model_selection import KFold
kfold = KFold(n_splits = 3)
print("교차 검증 점수: \n", cross_val_score(logreg, iris.data, iris.target, cv = kfold)
```
아까 분꽃 데이터에서 기본 k-겹 교차 검증을 사용하면 말도 안되는 정확도가 나온다고 했습니다. 물론 계층별 폴드를 만들 수도 있지만, 이 문제를 해결하는 다른 방법은 데이터를 섞어 샘플의 순서를 뒤죽박죽으로 만드는 것입니다. 바로 **shuffle** 매개변수를 True로 바꾸어주면 됩니다. 이때 random_state를 고정해서 똑같은 작업을 재현할 수 있습니다. 

```python
from sklearn.model_selection import KFold
kfold = KFold(n_splits = 3, shuffle = True, random_state =0)
print("교차 검증 점수: \n", cross_val_score(logreg, iris.data, iris.target, cv = kfold)
```
<br>
2. LOOCV

LOOCV, Leave-one-out cross-validation 역시 또 다른 교차 검증 방법입니다. LOOCV는 각 반복에서 하나의 데이터 포인트를 선택해 테스트 세트로 사용합니다. 폴드 하나에 샘플 하나만 들어 있는 k-겹 교차 검증으로 생각하면 쉬울 것 같습니다. 이는 작은 데이터셋에서 유용한 결과를 만들어 냅니다.

<br>
3. 임의 분할 교차 검증

임의 분할 교차 검증은 매우 유연한 또 하나의 교차 검증 전략으로 알려져 있습니다. 이 교차 검증에서는 train_size만큼의 포인트로 훈련 세트를 만들고, test_size만큼의 테스트 세트를 만듭니다. 이때 두 세트의 데이터 포인트는 중복되지 않습니다. 이러한 분할은 n_splits 횟수만큼 반복됩니다. 
아래의 그림은 샘플이 10개인 데이터 셋을 5개의 훈련 세트, 2개의 테스트 세트로 선택한 것입니다.

![](https://images.velog.io/images/drizzle0171/post/3a85fc7f-559e-4d4c-a88d-d8f4569b0062/image.png)

```python
from sklearn.model_selection import ShuffleSplit
shuffle_split = ShuffleSplit(test_size = 5, train 5, n_splits = 10)
```
위의 그림처럼 선택되지 않는 데이터들도 있고, 코드와 같이 50%의 비율로 선택할 수도 있습니다.

<br>
3. 그룹별 교차 검증

데이터 안에 매우 연관된 그룹이 있을 때도 교차 검증을 널리 사용합니다. 예를 들어 표정을 인식하는 시스템을 만들기 위해 100명의 얼굴 사진을 모았다고 합시다. 한 사람이 각기 다른 표정으로 여러 장을 찍었고, 이 데이터셋에 없는 사람의 표정을 정확히 구분할 수 있는 분류기를 만드는 것이 목표입니다. 

물론 계층별 k-겹 교차 검증을 사용할 수도 있지만, 이렇게 되면 테스트 세트에 같은 얼굴이 들어갈 수 있습니다. 이렇게 되면 모델의 정확도는 높일 수 있지만, 우리의 궁극적인 목표는 새로운 얼굴에 대한 식별이므로 일반화 성능을 높이는데 집중 해야합니다.

이를 위해 사진의 사람이 누구인지를 기록한 배열 groups 매개변수로 전달받을 수 있는 GroupKFold를 사용할 수 있습니다. (groups 배열과 클래스 혼동하지 않기!)

```python
from sklearn.model_selection import GroupKFold
X, y = make_blobs(n_samples = 12, random_state = 0)
groups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]
score = cross_val_score(logreg, X, y, groups = groups, cv = GroupKFold(n_splits = 3))
```
이를 그림으로 나타내면 아래와 같습니다. 

![](https://images.velog.io/images/drizzle0171/post/853c25c4-27f7-4db2-8066-9d09c5b0c19d/image.png)

## 4. 반복 교차 검증
만일 데이터셋이 크지 않다면 안정된 검증 점수를 얻기 위해 교차 검증을 반복해서 수행할 수 있습니다. 이를 위해 scikit-learn 0.19 버전부터 RepeatedKFold(회귀)와 RepeatedStratifiedKFold(분류) 분할기가 추가되었습니다. 이 클래스의 객체를 cross_val_score에 cv 매개변수로 전달하여 교차 검증을 반복할 수 있습니다. 
- 분할 폴드 수: n_splits 매개변수 (기본값 = 5)
- 반복 횟수: n_repeats 매개변수 (기본값 = 10)
(반복할 때마다 데이터를 다시 섞습니다.)

# 2. 그리드 서치

앞서 우리는 모델의 성능을 평가하는 방법을 배웠습니다. 그럼 그 평가 점수를 기반으로 우리는 더 좋은 성능을 낼 수 있는 모델을 만들어야겠죠? 이때 매개변수를 튜닝하여 일반화 성능을 개선해보도록 하겠습니다.

많이 하는 작업이라, scikit-learn에서는 이를 위한 메서드가 준비되어 있습니다. 그리드 서치는 그 중에서도 가장 많이 사용하는 방법인데요, 정의는 다음과 같습니다.

> 관심 있는 매개변수들을 대상으로 가능한 모든 조합을 시도해보는 작업

<br>

## 1. 간단한 그리드 서치
두 매개변수 조합에 대해 분류기를 학습시키고 평가하는 간단한 그리드 서치를 for 문을 사용해 만들 수 있습니다.
SVC 파이썬 클래스에 구현된 RBF 커널 SVM에서는 gamma와 C가 중요한데, 이를 for 문을 통해 구현해보면,
```python 
best_score = 0
for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:
	for C in [0.001, 0.01, 0.1, 1, 10, 100]:
    	svm = SVC(gamma = gamma, C = C)
        svm.fit(X_train, y_train)
        score = svm.score(X_test, y_test)
        if score > best_score:
        	best_score = score
            best_parameters = {'C': C, 'gamma':gamma}
 ```
 
 
<br>

## 2. 매개변수 과대적합과 검증 세트
 앞서 소개한 방법은 이 방법이 새로운 데이터에 이어지지 않을 수 있습니다. 매개변 
수를 조정하기 위해 테스트 세트를 이미 사용했기 때문에 모델이 얼마나 좋은지 **평가**할 수 없습니다. 즉, 평가를 위해서는 모델을 만들 때 사용하지 않은 독립된 데이터셋이 필요합니다.

이 문제를 해결하기 위해 우리는 데이터를 세 세트로 나눌 수 있습니다.

1. 훈련 세트
2. 검증 또는 개발을 목적으로 한 세트 (매개변수 선택)
3. 테스트 세트 (매개변수 성능 평가)

이렇게 훈련 세트, 검증 세트, 테스트 세트의 구분은 실제 머신러닝 알고리즘을 적용하는 데 아주 중요합니다. 테스트 세트 정확도에 기초해 어떤 선택을 했다면, 테스트 세트의 정보를 모델에 누설한 것입니다. 그렇기 때문에 최종 평가에만 사용하도록 테스트 세트를 분리하는 것이 중요한 것이죠. 

## 3. 교차 검증을 사용한 그리드 서치
데이터를 위와 같이 세 세트로 나눈 방법은 매우 민감합니다. 일반화 성능을 더욱이 잘 평가하기 위해서는 훈련 세트와 검증 세트를 한 번만 나누지 않고 앞서 배운 **교차 검증**을 사용하여 각 매개변수 조합의 성능을 평가할 수 있습니다. 

```python
for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:
	for C in [0.001, 0.01, 0.1, 1, 10, 100]:
		svm = SVC (gamma = gamma, C = C)
        scores = cross_val_score(svm, X_trainval, y_trainval, cv = 5)
        score = np.mean(scores)
            if score > best_score:
            	best_score = score
                best_parameters = {'C': C, 'gamma': gamma}
```

여기서 5-겹 교차 검증과 C, gamma를 다양한 변수로 설정하였으므로 총 36 * 5 = 180 개의 모델을 만들어야 합니다. 단점은 앞선 모델 평가 방식보다 시간이 더 걸린다는 점이겠죠.

![](https://images.velog.io/images/drizzle0171/post/4711e2ee-97ef-4bbb-b30d-ace369c30aba/image.png)

이 그림은 최적의 매개변수를 설정하는 방법을 보여줍니다. 여기서  교차 검증 정확도, 즉 validation accuracy가 가장 높은 매개변수를 동그라미로 표시하였습니다. 책의 추가 설명을 빌리자면, 교차 검증은 데이터셋에 대해 주어진 알고리즘을 평가하는 방법입니다. 하지만 그리드 서치와 같은 매개변수 탐색 방법과 합쳐서 많이 사용합니다. 그래서 많은 사람이 교차 검증이란 용어를 교차 검증을 사용한 그리드 서치라는 의미로 주로 사용합니다.

앞서 설명한 내용을 도식화 하면 아래와 같습니다.

![](https://images.velog.io/images/drizzle0171/post/81ac81e1-e59f-4b39-96d5-cfd0b019819d/image.png)

scikit-learn의 좋은 점 중 한 가지는 널리 사용하는 방법을 친절히 제공해준다는 것입니다. 교차 검증을 사용한 그리드 서치를 매개변수 조정 방법으로 널리 사용하기 때문에, 추정기 형태로 구현된 GridSearchCV를 제공하고 있습니다.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}

grid_search = GridSearchCV(SVC(), param_grid, cv=5, return_train_score = True)
```

매개변수가 과대적합이 되길 방지하기 위해서는 훈련 세트와 검증 세트를 나누는 것이 좋겠죠? 

가끔은 만들어진 실제 모델에 직접 접근해야 할 때가 있는데요, 예를 들면 계수나 특성 중요도를 살펴보려고 할 때입니다. 최적의 매개변수에서 전체 훈련 세트를 사용하여 학습한 모델은 best_estimator_ 속성에서 얻을 수 있습니다. (모델 평가를 위해 best_estimator을 사용할 필요는 없습니다.)


<br>
1. 교차 검증 결과 분석

교차 검증의 결과를 시각화 하면 검색 대상 매개변수가 모델의 일반화에 영향을 얼마나 주는지 이해하는 데 도움이 됩니다. 그리드 서치는 연산 비용이 커, 비교적 간격을 넓게 하여 적은 수의 그리드로 시작하는 것이 좋습니다. 그리드 서치의 결과는 검색과 관련한 여러 정보가 함께 저장되어 있는 딕셔너리인 cv_results_ 속성에 담겨 있습니다.

```python
import pandas as pd
pd.set_option('disply.max_columns', None)
results = pd.DataFrame(grid_search.cs_results_)
display(np.transpose(results.head())
```
results의 행 하나에는 특정한 하나의 매개변수 설정에 대응합니다. 각 설정에 대해 교차 검증의 모든 분할의 평균값, 표준편차를 포함한 결과가 기록되어 있습니다. 매개변수 그리드가 2차원이므로 히트맵으로 시각화 하기 좋습니다.

![](https://images.velog.io/images/drizzle0171/post/4a6391b6-4b91-4086-bf68-b8b3386d620d/image.png)

여기서 교차 검증의 정확도가 높으면 밝은 색으로 낮으면 어두운 색으로 나타내었습니다. 이 그래프를 보면 SVC가 매개변수 설정에 매우 민감함을 알 수 있습니다. 

여기서 알 수 있는 사실은 매개변수 조정에 따라 정확도가 크게 달라지고, 각 매개변수의 최적값이 그래프 끝에 놓이지 않도록 매개변수의 범위가 충분히 넓게 설정해야 한다는 점입니다.

만약 매개변수의 범위를 넓게 설정하지 않는다면,

![](https://images.velog.io/images/drizzle0171/post/aa5faa6a-ee5d-4e1a-93a8-e8a1022675b1/image.png)

이와 같은 결과를 볼 수 있습니다. 

<BR>
<BR>
2. 비대칭 매개변수 그리드 탐색

어떤 경우에는 모든 매개변수의 조합에 대해 GridSearchCV를 수행하는 것이 좋지 않을 수 있습니다. 예를 들면 SVC는 kernel 매개변수를 가지고 있는데 어떤 커널을 사용하는지에 따라 관련 있는 매개변수들이 결정됩니다.
  - linear = C
  - rbf = C, gamma
  
시간 낭비하는 것을 줄이기 위해서 우리는 조건부 매개변수 조합을 적용하여야 합니다. 이는 GridSearchCV에 전달할 param_grid를 딕셔너리의 리스트로 만들어주면 됩니다. 이때, 각 딕셔너리를 독립적인 그리드로 적용됩니다. 

```python
param_grid = [{'kernel': ['rbf'], 
  				'C': [0.001, 0.01, 0.1, 1, 10, 100],
  				'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},
              {'kernel': ['linear'],
  				'C': [0.001, 0.01, 0.1, 1, 10, 100]}]
```

<BR>
<BR>
3. 그리드 서치에 다양한 교차 검증 적용

cross_val_score와 비슷하게, GridSearchCV는 분류에는 기본적으로 계층형 k-겹 교차 검증을 사용하고 회귀에는 k-겹 교차 검증을 사용합니다. 또, 훈련 세트와 검증 세트로 한 번만 분할하려면 n_splits = 1로 하고 ShuffleSplit나 StratifiedShffleSplit를 사용합니다. 이런 방법은 데이터셋이 매우 크거나 모델 구축에 시간이 오래 걸릴 때 유용합니다.
  
<br>
<br>
4. 중첩 교차 검증

GridSearchCV를 사용할 때 여전히 데이터를 훈련 세트와 테스트 세트로 한 번만 나누기 때문에, 결과가 불안정하고 테스트 데이터의 분할에 크게 의존합니다. 여기서도 물론 교차 검증 분할 방식을 사용할 수 있습니다. 이를 **중첩 교차 검증**이라고 합니다.

중첩 교차 검증에서는 바깥쪽 루프에서 데이터를 훈련 세트와 테스트 세트로 나눕니다. 그 후 각 훈련 세트에 대해 그리드 서치를 실행합니다. 마지막으로 분할된 테스트 세트의 점수를 최적의 매개변수 설정을 사용해 각각 측정합니다.
  
이 방법은 모델이나 매개변수 설정이 아닌 테스트 점수의 목록을 만들어줍니다. 그래서 중첩 교차 검증은 미래의 데이터에 적용하기 위한 예측 모델을 찾는 데는 거의 사용하지 않습니다. 
  
<br>
<br>
5. 교차 검증과 그리드 서치 병렬화

  그리드 서치는 데이터 용량이 크고 매개변수 수도 많을 때는 상당한 연산 부하를 일으킵니다. 하지만 쉽게 병렬화할 수 있다는 장점이 있습니다. 하나의 교차 검증 분할에서 특정 매개변수 설정을 사용해 모델을 만드는 일은 다른 매개변수 설정이나 모델과 전혀 상관없이 진행할 수 있기 때문입니다. GridSearchCV와 cross_val_score에서 n_jobs 매개변수에 사용할 CPU 코어 수를 지정할 수 있습니다. n_jobs = -1이면 가능한 모든 코어를 사용합니다. 
# 평가 지표
지도 학습 모델의 성능을 재는 평가 지표는 크게 이진 분류의 평가 지표와 다중 분류의 평가 지표로 나뉩니다.

## 이진 분류의 평가 지표
이진 분류에는 양성 클래스와 음성 클래스가 있습니다.
<br>
> - 에러의 종류
	1. **거짓 양성** : 음성인데 양성으로 잘못 분류
	2. **거짓 음성** : 양성인데 음성으로 잘못 분류
    
<br>

**불균형 데이터셋** : 한 클래스가 다른 것보다 훨씬 많은 데이터셋!

### 오차 행렬

오차행렬은 이진 분류 평가 결과를 나타낼 때 가장 널리 사용하는 방법 중 하나입니다. 

- 2x2배열의 행은 정답 클래스, 열은 예측 클래스에 해당합니다. 
- 각 항목의 숫자는 행의 클래스가 얼마나 많이 열의 클래스로 분류되었는지 나타냅니다.
- 대각 행렬은 정확히 분류된 경우, 다른 항목은 한 클래스의 샘플들이 다른 클래스로 잘못 분류된 경우입니다.

![image](https://images.velog.io/images/hanseulgi/post/9eb8c5bd-a64f-46a7-926d-764ffa73b5b5/KakaoTalk_20211116_121226739_01.jpg)

-- '9 아님'을 양성 클래스라고 했을 때 --
위 그림에서 9가 아니라고 예측했는데 실제로는 9가 아니었던 샘플이 401개입니다. (진짜 양성)
또 실제로 9였던 샘플이 8개 입니다. (거짓 양성)

> 양성 클래스를 올바르게 분류한 샘플을 **진짜 양성(TP)**
양성 클래스를 잘못 분류한 샘플을 **거짓 양성(FP)**
음성 클래스를 올바르게 분류한 샘플을 **진짜 음성(TN)**
음성 클래스를 잘못 분류한 샘플을 **거짓 음성(FN)**

오차 행렬의 결과를 요약하는 방법 중 하나는 **정확도**로 표현하는 것입니다.
- 정확도 = TP+TF / TP+FP+TN+FN 
<br>

### 정밀도, 재현율, f-점수

정확도뿐만 아니라 정밀도, 재현율, f-점수로도 오차 행렬의 결과를 요약할 수 있습니다.


1. **정밀도**(양성 예측도) = 𝑇𝑃/(𝑇𝑃+𝐹𝑃) 		
양성으로 예측된 것 중에 얼마나 많은 샘플이 진짜 양성인지 측정

2. **재현율**(민감도, 적중률, 진짜 양성 비율) = 𝑇𝑃/(𝑇𝑃+𝐹𝑁)		
전체 양성 샘플 중 진짜 양성으로 올바르게 분류된 샘플의 비율.

- 정밀도 최적화와 재현율 최적화는 상충함
if 모든 샘플이 양성(TP 또는 FP)으로 분류되면
TN과 FN은 0이 되고 따라서 재현율은 1로 완벽함
그러나 FP값이 커지므로 정밀도는 작아짐

f-점수는 정밀도와 재현율을 다시 하나로 요약해줍니다.

3. **f-점수** : 정밀도와 재현율의 조화 평균
F(f1-점수) = 2×(정밀도∙재현율)/(정밀도+재현율)
<br>

#### 임계값
양성 확률이 임계 값보다 크거나 같으면 샘플은 양성(TP, NP)로 분류 
-> 임계값 낮으면 양성 클래스 비율 높아짐
-> 임계값 높으면 음성 클래스 비율 높아짐
![](https://images.velog.io/images/hanseulgi/post/a56ddd03-7c66-4e14-a5cc-7dffa43c5c44/%EA%B7%B8%EB%A6%BC1.png)
<br>

### 정밀도-재현율 곡선
모든 임계값을 조사해보거나, 한 번에 정밀도나 재현율의 모든 장단점을 살펴봄
    
``` python
from sklearn.metrics import precision_recall_curve 
``` 

- 타깃 레이블과 decision_function이나 predict_proba 메서드로 계산한 예측 불확실성 이용

![](https://images.velog.io/images/hanseulgi/post/3a9632aa-96e3-4198-97a5-f97859a5149f/KakaoTalk_20211116_121226739_02.jpg)

- 곡선의 각 포인트는 decision_function의 가능한 모든 임계값에 대응함
- 기본 임계값인 0지점 기준으로 임계값이 변함에 따라 재현율과 정밀도가 상충함을 알 수 있음
- 곡선이 정밀도와 재현율이 모두 높은 지점인 오른쪽 위로 갈수록 더 좋은 분류기임


전체 곡선에 담긴 정보를 요약하기 위해 곡선의 아랫부분 면적을 계산하여 **평균 정밀도**를 구할 수 있습니다.
<br>

### ROC, AUC

**ROC곡선**은 여러 임계값에서 분류기의 특성을 분석하는 데 널리 사용하는 도구입니다.

정밀도-재현율 곡선처럼 분류기의 모든 임계값을 고려하지만, 정밀도와 재현율 대신 **진짜 양성 비율(재현율)에 대한 거짓 양성 비율**을 나타냅니다.
- 진짜 양성 비율(TPR) = 재현율
- 거짓 양성 비율 : 전체 음성 샘플 중 거짓 양성으로 잘못 분류한 비율, FP / FP+TN

``` phthon
from sklearn.metrics import roc_curve
```

![](https://images.velog.io/images/hanseulgi/post/569e5e09-4d48-44aa-8aa1-7ea12e8be17b/KakaoTalk_20211116_121226739_04.jpg)

- Roc곡선은 FPR이 낮게 유지되면서 TPR 이 높은 왼쪽 위에 가까울수록 좋음
- 기본 임계값 0인 지점에서 FPR을 좀 늘려서 재현율을 크게 높일 수 있음
<br>

ROC에서도 마찬가지로 곡선 아래의 면적값을 이용해서 ROC곡선을 요약할 수 있습니다. 

이 면적을 보통 **AUC**라고 합니다.

- AUC는 0과 1 사이의 값
- 데이터의 불균형에 상관없이 무작위로 예측한 AUC값은 0.5 가 됩니다.
- 그래서 불균형한 데이터셋에서는 정확도보다 AUC가 훨씬 좋습니다.
<br>

``` phthon
from sklearn.metrics import roc_auc_curve
```

![](https://images.velog.io/images/hanseulgi/post/654b472a-5a23-45a2-9b12-a2feac9bb3e3/KakaoTalk_20211116_121226739_06.jpg)

서로 다른 gamma에 대한 SVM의 ROC 곡선입니다.
(지금까지의 곡선은 모두 SVM으로 그린 곡선입니다.)

*※ 세가지 감마값에 대한 정확도는 모두 0.9입니다.*

> gamma=1 일 때 AUC=0.5
gamma=0.1 일 때 AUC=0.94
gamma=1 일 때 AUC=1

그래서 불균형 데이터셋에는 AUC를 사용하는 게 좋습니다.
<br>

## 다중 분류의 평가 지표

다중 분류의 평가 지표는 기본적으로 방금까지 살펴본 이진 분류 평가 지표에서 유도되었지만, 모든 클래스에 대해서 **평균을 낸 것**입니다. 

다중 분류의 정확도 또한 전체 샘플분의 정확히 분류된 샘플의 비율로 계산합니다. 


다중 분류에서 불균형 데이터셋을 위해 가장 널리 사용하는 평가 지표는 다중 분류 버전의 f1- 점수입니다.

다중 클래스용 f1-점수는 한 클래스를 양성, 나머지를 음성으로 간주하여 클래스마다 f1-점수를 계산하고 클래스별 점수를 평균을 냅니다.

평균을 내는 방법은 여러가지입니다.
> **macro 평균** : 클래스별 f1-score에 가중치를 주지 않음. 합의 평균
**weighted 평균** : 클래스별 샘플 수로 가중치를 두어 f1-score 점수의 평균을 계산
**micro 평균** : 모든 클래스의 FP, FN, TP의 총 수로 정밀도, 재현율, f1-score를 계산

<br>

### 회귀의 평가 지표

회귀의 평가 지표는 **R^2**으로 충분합니다.

<br>

지금까지 ML의 9주차 학습 내용이었습니다!
어느덧 저희의 주교재도 끝나가는데요, 정말 멋집니다 ~!~!
앞으로도 더욱더 파이팅!!!
