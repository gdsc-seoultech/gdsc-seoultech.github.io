<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="tYMwPoxNrWfLx2sDrDYzqI4-dq3M3FfI56NBL9JNtH8" />

    <title>4주차 ML 지도학습 WIL</title>
    <meta name="description" content="A simple, whitespace, helvetica based portfolio theme.
">

    <link rel = "shortcut icon" type="image/x-icon" href="/img/square_logo.png">
    <link rel="stylesheet" href="/css/main.css">
<!--    <link rel="canonical" href="http://localhost:4000/posts/1st_term/ml/2021-10-03-ml_wil_uj/">-->
    <link rel="canonical" href="/posts/1st_term/ml/2021-10-03-ml_wil_uj/">

    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- 카테고리 css -->
    <link rel="stylesheet" href="/css/pagination.css">
    <link href="/css/category.css" type="text/css" rel="stylesheet">
</head>

   
  <body>
    <script src="/js/default.js"></script>
    <header id="tab" >
  <script src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
  <div class="site-header">
    <div class="wrapper">
      <div>
        <a href="/">
          <img src="/img/square_logo.png" align="left" width="57px">
        </a>
      </div>

      <nav class="site-nav">

        <div id="tab" class="trigger">
          <!-- GDSC Seoultech instead of blog -->
          <a class="page-link" href="/">GDSC</a>
          <a class="page-link" href="/members/2">MEMBER</a>
          <a class="page-link" href="/category/2nd_term">POST</a>
        </div>
      </nav>
    </div>
  </div>

  <div id="hovering" class="tab-header">
    <div class="wrapper">
      <div class="detail-post">
          <div class="tab-item">
            <div><a class="page-link-detail" href="/category/1st_term">1기</a></div>
            <div><a class="page-link-detail" href="/category/2nd_term">2기</a></div>
          </div>
      </div>
      <div class="detail-member">
        <div class="tab-item">
          <div><a class="page-link-detail" href="/members/1">1기</a></div>
          <div><a class="page-link-detail" href="/members/2">2기</a></div>
        </div>
    </div>
    </div>

  </div>

  <script type="text/javascript">
    var tabMenu = $("#tab");
    var tabSubMenu = $("#hovering");

    tabSubMenu.hide()

    tabMenu.hover(function() {
      tabSubMenu.show();
    }, function() {
      tabSubMenu.hide();
    })
  </script>

</header>


    <div class="page-content" id="page-content">
      <div class="wrapper" id="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">4주차 ML 지도학습 WIL</h1>
    <h1 class="post-description"></h1>
    <p class="post-meta">October 3, 2021 — 00:00 • juijeong8324</p>
    
      
        <span class="tag">ml</span>
      
    
  </header>
  <article class="post-content">
    <p>저는 어차피 머신러닝을 공부할꺼 제대로 공부해보자 다짐하고 WIL이지만 블로그 글처럼 써보려고 합니다….(스압 주의^^)</p>

<h2 id="235-결정트리-"><span style="color:#1E90FF">2.3.5 결정트리 </span></h2>
<p><img src="https://user-images.githubusercontent.com/63052097/135849599-caa40532-aee2-4360-ba85-e72274519c21.png" width="500" /></p>

<ul>
  <li><strong>개념</strong> <br />
◽ <strong>결정 트리(decision tree)</strong> : 특정 기준(질문)에 따라 데이터를 구분하는 모델을 결정 트리 모델 <br />
◽ <strong>노드</strong> : 질문이나 정답을 담은 네모 상자     <br />
<span style="padding: 20px">▪ 리프(leaf) : 마지막 노드</span>  <br />
<span style="padding: 20px">▪ 루프 노드(root node) : 맨 위의 노드, 전체 데이터 셋과 처음 분류 기준이 포함 </span> <br />
<span style="padding: 20px">▪  순수 노드(pure node) : 한 개의 타깃 값(하나의 클래스나 하나의 회귀 분석 결과)로 이루어진 리프 노드</span>      <br />
◽ <strong>에지(edge)</strong> : 질문의 답과 다음 질문을 연결 <br />
◽ <strong>테스트</strong> : 결정 트리에 있는 질문들 (테스트 세트와 혼동하지 않기)</li>
</ul>

<p><br />
<br /></p>

<h3 id="결정트리-만들기">결정트리 만들기</h3>
<p><br />  <br />
<img src="https://user-images.githubusercontent.com/63052097/135851944-08d03627-d4bc-43ca-a9af-0eead32d28ad.png" /></p>

<p><strong>STEP1</strong>. 모든 테스트에서 타깃 값에 대해 가장 많은 정보를 가진 특성을 이용해 테스트(x[1]&lt;=0.06)한다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/63052097/135852334-3d9d2f1c-dca3-4b34-a501-9b3ea16b574c.png" /></p>

<p><strong>STEP2</strong>. 각 노드에 대해 데이터를 잘 구분할 수 있는 테스트를 추가한다. 이 때 결정 트리의 리프에 한 개의 타깃 값(여기서는 클래스)을 가질 때 즉, 순수 노드가 될 때까지 반복한다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/63052097/135852875-1da9a388-4234-4144-86e4-901e2b0a820b.png" /></p>

<p><strong>STEP3</strong>. 최종 분할 트리의 모습!</p>

<p><br /></p>

<p><strong>STEP4</strong>. 새로운 데이터 포인트에 대해 예측 <br />
 <span style="padding: 20px"> ◽ 분류  </span>  <br />
 <span style="padding: 20px">   : 주어진 데이터 포인트가 특성을 분할한 영역 중 어디에 놓이는가?   </span><br />
 <span style="padding: 20px">   : 영역의 타깃 값 중 다수(순수 노드라면 하나)인 것을 예측 결과로 한다 </span></p>

<p><span style="padding: 20px"> ◽ 회귀 </span> <br />
 <span style="padding: 20px">  : 각 노드의 테스트 결과에 따라 트리를 탐색하고 찾은 리프 노드의 훈련 데이터 평균값을 예측 결과로 한다  </span></p>

<p><br /></p>

<h3 id="결정트리의-복잡도-제어하기">결정트리의 복잡도 제어하기</h3>

<p><span style="background:#FFEBCD;font-weight:bold"> 1) 문제점 </span> <br />
<img src="https://user-images.githubusercontent.com/63052097/135854224-7e1ed67e-f66a-49fe-8147-b03d44051d46.png" /></p>
<ul>
  <li>모든 리프 노드가 순수 노드가 될 때까지 진행하면 <span style="color:red;font-weight:bold">모델이 복잡해지고 훈련 데이터에 과대적합 되어 새로운 데이터에 일반화 되지 않는다.</span></li>
  <li>결정경계가 클래스의 포인트들에서 멀리 떨어진 이상치에 민감.(클래스 0으로 결정된 영역이 클래스 1에 속한 포인트들로 둘러쌓인 이상치까지 판별하고 있음)</li>
</ul>

<p><br /></p>

<p><span style="background:#FFEBCD;font-weight:bold"> 2) 해결방법 </span></p>
<ul>
  <li>
    <dl>
      <dt><strong>사전 가지치기</strong></dt>
      <dd>트리 생성을 일찍 중단하는 전략</dd>
      <dd><strong>트리의 최대 깊이</strong>나 <strong>리프의 최대 개수</strong>를 제한</dd>
      <dd>노드가 분할하기 위한 포인트의 최소 개수를 지정</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong>사후 가지치기(가치지기)</strong></dt>
      <dd>트리를 만든 후 데이터 포인트가 적은 노드 삭제 혹은 병합</dd>
    </dl>
  </li>
</ul>

<p><br /></p>

<p><strong>참고</strong> <br />
scikit-learn에서 결정트리는 DecisionTreeRegressor와 DecisionTreeClassifier에 구현 <br />
sckit-learn은 사전 가지치기만 지원</p>

<p><br /></p>

<p><span style="background:#FFEBCD;font-weight:bold"> 3) 실습- 유방암 데이터셋을 이용하여 사전 가지치기의 효과를 확인해보자! </span></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span> <span class="c1"># 결정 트리 가져오기 
</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>   

<span class="c1"># 훈련 세트와 테스트 세트로 나누기
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1">#기본값 설정으로 완전한 트리 모델 생성
</span><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#random_state=0으로 하여 트리를 같은 조건으로 비교
</span><span class="n">tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># 훈련시킨다! 
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>훈련 세트 정확도: 1.000
테스트 세트 정확도: 0.937
</code></pre></div></div>
<p>✅ <strong>Result</strong> <br />
모든 리프 노드가 순수 노드이기 때문에 훈련 세트의 정확도는 100% <br />
테스트 세트의 정확도는 선형 모델에서의 정확도보다 낮다</p>

<p><br />
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 결정 트리의 깊이 제한으로 과대 적합 줄이기 
</span><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#max_depth로 제한 -&gt; 연속된 질문 최대 4개로 제한 
</span><span class="n">tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>훈련 세트 정확도: 0.988
테스트 세트 정확도: 0.951
</code></pre></div></div>
<p>✅ <strong>Result</strong>  <br />
훈련 세트의 정확도를 떨어뜨리지만 테스트 세트의 성능은 개선!</p>

<p><br />
<br /></p>

<h3 id="결정-트리-시각화">결정 트리 시각화</h3>
<p><span style="background:#FFEBCD;font-weight:bold"> 1) 시각화 </span></p>
<ul>
  <li>tree 모듈의 export_graphviz 함수를 이용함</li>
  <li>그래프 저장용 텍스트 파일 포맷인 .dot 파일 생성</li>
  <li>export_graphviz 함수에 filled 매개변수를 True로 지정하면 노드의 클래스가 구분되도록 칠해준다<br />
  <br /></li>
</ul>

<p><span style="background:#FFEBCD;font-weight:bold"> 2) 실습 </span></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span> 
 <span class="c1"># 결정 트리, 생성되는 파일이름, 클래스이름, 특성 이름, filled는 노드에 색이 칠해짐
</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="s">"tree.dot"</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s">"악성"</span><span class="p">,</span> <span class="s">"양성"</span><span class="p">],</span>
                <span class="n">feature_names</span><span class="o">=</span><span class="n">cancer</span><span class="p">.</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">impurity</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kn">import</span> <span class="nn">graphviz</span> <span class="c1"># graphviz 모듈을 사용해 시각화!
</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"tree.dot"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">dot_graph</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">graphviz</span><span class="p">.</span><span class="n">Source</span><span class="p">(</span><span class="n">dot_graph</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># plot_tree() 함수를 사용하면 .dot 파일을 만들지 않고 바로 트리를 그릴 수 있음!
</span> <span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span>

<span class="n">plot_tree</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s">"악성"</span><span class="p">,</span> <span class="s">"양성"</span><span class="p">],</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">cancer</span><span class="p">.</span><span class="n">feature_names</span><span class="p">,</span>
         <span class="n">impurity</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/63052097/135858752-dafbafdf-13c6-4360-860c-c68e0bb967de.png" width="500" /></p>

<p><br />
 <br /></p>

<h3 id="트리의-특성-중요도">트리의 특성 중요도</h3>
<p><span style="background:#FFEBCD;font-weight:bold">1) 특성 중요도(feature importance) </span></p>
<ul>
  <li>트리를 만드는 결정에 각 결정에 각 특성이 얼마나 중요한지 평가</li>
  <li>0과 1 사이의 양수(0: 전혀 사용되지 않았다 / 1: 완벽하게 타깃 클래스를 예측)</li>
  <li>트리가 너무 커서 전체 트리를 살펴보는게 어려울 때 확인</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"특성 중요도:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">tree</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>특성 중요도:
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.048
 0.    0.    0.002 0.    0.    0.    0.    0.    0.727 0.046 0.    0.
 0.014 0.    0.018 0.122 0.012 0.   ]
</code></pre></div></div>
<p><br /> <br />
이를 시각화 해보면</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_feature_importances_cancer</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">cancer</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">barh</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_features</span><span class="p">),</span> <span class="n">model</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s">'center'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_features</span><span class="p">),</span> <span class="n">cancer</span><span class="p">.</span><span class="n">feature_names</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"특성 중요도"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"특성"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>

<span class="n">plot_feature_importances_cancer</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/63052097/135860109-9cf3a232-bb3c-43f5-9637-4c570f663211.png" alt="image" /></p>

<p>✅ <strong>분석</strong> <br />
worst radius(첫 번째 노드에서 사용한 특성)가 가장 중요한 특성임 <br />
-&gt; 첫 번째 노드에서 두 클래스를 잘 나누고 있다. <br />
<br /></p>

<p><span style="background:#FFEBCD;font-weight:bold">2) 한계 </span></p>
<ul>
  <li>
    <p>특성 중요도는 <span style="color:red;font-weight:bold">해당 특성이 어떤 클래스를 지지, 결정하는지 알 수 없다.(이건 트리를 봐야 함)</span>
EX) worst radius가 중요하다고 알려주지만 높은 반지름이 양성(클래스0)인지 악성(클래스1)인지 의미할 수 없다. <br />
cf) 특성과 클래스 사이의 관계는 단순하지 않고 복잡할 수 있음</p>
  </li>
  <li>
    <p>회귀 결정 트리, 즉 DecisionTreeRegressor는 <span style="color:red;font-weight:bold">외삽(훈련 데이터의 범위 밖의 새로운 데이터 포인트)에 대해 예측할 수 없음</span></p>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/63052097/135861416-281964ee-975b-4a2e-ae82-69a4b7482fed.png" width="500" /></p>

<p>✅ <strong>분석</strong> <br />
선형 모델은 테스트 데이터를 꽤 정확히 예측하지만 트리 모델은 훈련 데이터를 완벽하게 예측한다. 그러나 모델이 가진 데이터 범위 밖으로 나가면 마지막 포인트를 이용해 예측..<br />
<br />
<br /></p>

<h3 id="장단점과-매개변수">장단점과 매개변수</h3>
<p><strong>👍장점</strong> <br />
첫째, 만들어진 모델을 쉽게 시각화 가능 <br />
둘째, 데이터 스케일에 구애받지 않아 결정 트리에서 특성의 정규화나 표준화 같은 전처리 과정이 필요 없음(이진 특성과 연속적인 특성이 혼합되어도 잘 작동)</p>

<p><strong>👎단점</strong>   <br />
사전 가지치기를 사용해도 과대적합되는 경향이 있어 일반화 성능이 좋지 않다.<br />
<br />
<br /></p>

<h3 id="보충">보충</h3>
<p>엔트로피와 불순도에 대해서 추후.. 작성하도록 하겠음..  <br />
<br />
<br /></p>

<h2 id="236-결정-트리의-앙상블"><span style="color:#1E90FF">2.3.6 결정 트리의 앙상블</span></h2>
<p><strong>앙상블</strong> = 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법 <br />
<br /></p>

<p>다음은 분류와 회귀 문제의 다양한 데이터셋에서 효과적인 두 앙상블 모델</p>
<h3 id="랜덤-포레스트">랜덤 포레스트</h3>

<p><span style="background:#FFEBCD;font-weight:bold"> 1) 랜덤 🌲포레스트🌲?</span></p>
<ul>
  <li>결정 트리가 훈련 데이터에 과대적합된다는 단점을 해결!</li>
  <li>조금씩 다른 여러 결정 트리의 묶음</li>
  <li>서로 다른 방향으로 과대적합된 트리를 많이 만들고 그 결과를 평균냄 -&gt; 모델의 예측 성능이 유지되면서 과대적합이 줄어듬</li>
  <li>핵심 포인트!! 각 트리가 고유하게 만들어지도록 무작위한 선택을 함!! <br />
<br />
<br /></li>
</ul>

<p><span style="background:#FFEBCD;font-weight:bold">2) 랜덤 포레스트 구축</span></p>

<p><strong>STEP1) 생성할 트리의 개수 정하기</strong></p>
<ul>
  <li>RandomForestRegressor나 RandomForestClassifier의 n_estimators 매개변수 <br />
<br /></li>
</ul>

<p><strong>STEP2) 부트스트랩 샘플 생성</strong></p>
<ul>
  <li>트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택</li>
  <li>n_samples 개의 데이터 포인트 중에서 무작위로 테이터를 n_samples 횟수만큼 반복 추출(한 샘플이 여러 번 중복 추출될 수 있음)</li>
  <li>데이터 셋은 원래 데이터셋 크기와 같지만 모든 데이터 포인트가 들어있지 않다!(중복,, 누락,,)  <br />
<br /></li>
</ul>

<p><strong>STEP3) 분할 테스트에서 특성을 무작위로 선택</strong></p>
<ul>
  <li>max_features 매개변수로 고를 특성의 개수를 조정 <br />
<strong>❌주의❌</strong>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ◽ max_features = n_feautres로 설정하면 트리의 각 분기에서 모든 특성을 고려하므로 특성 선택에 무작위성이 들어가지 않는다   
 ◽ max_features = 1로 설정하면 트리의 분기는 테스트할 특성을 고를 필요가 없고 그냥 무작위로 선택한 특성을 활용    
 ◽ max_features 값을 크게 하면 랜덤 포레스트의 트리는 매우 비슷해지고 가장 두드러진 특성을 이용해 데이터에 맞춰질 것임!    
 ◽ max_features를 작게 하면 랜덤 포레스트 트리들은 많이 달라지고 각 트리는 깊이가 깊어진다(해당 특성에 데이터를 맞추기 위해서)   
</code></pre></div>    </div>
  </li>
  <li>각 노드에서 후보 특성을 무작위로 선택하고 이 후보들 중에서 최선의 테스트(타깃 값에 많은 데이터를 갖는 특성을 선택)를 찾는다.</li>
  <li>후보 특성을 고르는 것은 매 노드마다 반복되므로 트리의 각 노드는 다른 후보 특성들을 사용하여 테스트를 만든다. <br />
<br />
<br /></li>
</ul>

<p><span style="background:#FFEBCD;font-weight:bold">3) 회귀 VS 분류</span><br />
알고리즘이 모델에 있는 모든 트리의 예측을 만들고…</p>

<p><span style="padding: 20px"> ◽ 분류 : 약한 투표 전략을 사용 = 각 트리들이 예측한 확률을 평균내어 가장 높은 확률을 가진 클래스를 선정 </span> <br />
<span style="padding: 20px"> ◽회귀 : 예측들을 평균하여 최종 예측을 만듬  </span> <br />
<br />
<br /></p>

<p><span style="background:#FFEBCD;font-weight:bold">4) 실습 </span> <br />
트리 5개로 구성된 랜덤 포레스트 모델을 만들어 보자!!</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> 
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> 
<span class="c1">#트리를 5개 생성
</span><span class="n">forest</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 랜덤 포레스트 안에 만들어진 트리는 extimators_ 속성에 저장
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">tree</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">forest</span><span class="p">.</span><span class="n">estimators_</span><span class="p">)):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"트리 {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">mglearn</span><span class="p">.</span><span class="n">plots</span><span class="p">.</span><span class="n">plot_tree_partition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    
<span class="n">mglearn</span><span class="p">.</span><span class="n">plots</span><span class="p">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">forest</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">4</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"랜덤 포레스트"</span><span class="p">)</span>
<span class="n">mglearn</span><span class="p">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span> 
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/63052097/135869404-bd97dc6e-a607-43e8-843a-529a27538cdc.png" alt="image" /></p>

<p><strong>5) 매개변수</strong></p>
<ul>
  <li>
    <dl>
      <dt><span style="color:red;font-weight:bold">random_state </span></dt>
      <dd>지정하거나 지정하지 않으면 전혀 다른 모델이 만들어진다.</dd>
      <dd>랜덤 포레스트 트리가 많을 수록 random_state 값의 변화에 따른 변동이 적다</dd>
      <dd>같은 결과를 만들고 싶으면 random_state 값을 고정</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><span style="color:red;font-weight:bold">n_estimators </span></dt>
      <dd>클수록 좋다! 더 많은 트리를 평균하면 과대적합을 줄여 안정적인 모델을 만듬!</dd>
      <dd>단, 더 많은 메모리와 긴 훈련 시간으로 이어짐</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><span style="color:red;font-weight:bold">max_features </span></dt>
      <dd>각 트리가 얼마나 무작위가 될지를 결정</dd>
      <dd>일반적으로 기본값(auto)을 쓰는 것이 좋다</dd>
      <dd>분류 max_features=sqrt(n_features) / 회귀 max_features=n_features  <br />
<br />
<br /></dd>
    </dl>
  </li>
</ul>

<h3 id="그레이디언트-부스팅-회귀-트리">그레이디언트 부스팅 회귀 트리</h3>
<p><span style="background:#FFEBCD;font-weight:bold">1) 그레이디언트 부스팅 회귀 트리?</span></p>
<ul>
  <li>이전 트리의 예측과 타깃 값 사이의 오차를 줄이는 방향으로 순차적으로 새로운 트리를 추가하는 알고리즘 <br />
❗손실 함수를 정의 &amp; 경사 하강법을 사용하여 다음에 추가될 트리가 예측해야 할 값을 보정❗</li>
  <li>무작위성이 없으며 대신 강력한 사전 가지치기가 사용</li>
  <li>보통 하나에서 다섯 정도의 깊지 않은 트리를 사용 -&gt; 메모리를 적게 사용하고 예측도 빠름</li>
  <li>얕은 트리 같은 간단한 모델(약한 학습기)을 많이 연결 -&gt; 데이터 일부에 대해서 예측을 수행 <br />
-&gt; 트리가 많이 추가될수록 성능이 좋아짐</li>
  <li>회귀와 분류 모두 사용 가능 <br />
<br />
<br /></li>
</ul>

<p><span style="background:#FFEBCD;font-weight:bold">2) 실습</span></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 트리의 최대 깊이를 줄여 사전 가지치기를 강하게 함
</span><span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>훈련 세트 정확도: 0.991
테스트 세트 정확도: 0.972
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># 학습률을 낮춘다
</span><span class="n">gbrt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gbrt</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>훈련 세트 정확도: 0.988
테스트 세트 정확도: 0.965
</code></pre></div></div>

<p>✅ <strong>분석</strong> <br />
모델의 복잡도를 감소시키므로 훈련 세트의 정확도가 낮아짐 <br />
이 예시에서는 학습률을 낮추는 것이 테스트 세트의 성능을 조금밖에 개선 못함 반면, 트리의 최대 깊이를 낮추는 것은 모델 성능 향상에 크게 기여!! <br />
<br />
<br /></p>

<p><span style="background:#FFEBCD;font-weight:bold">3) 매개변수</span></p>
<ul>
  <li>
    <dl>
      <dt><span style="color:red;font-weight:bold">learning_rate</span></dt>
      <dd>이전 트리의 오차를 얼마나 강하게 보정할 것인지 보정 정도를 조절</dd>
      <dd>학습률이 크면 트리는 보정을 강하게 하여 복잡한 모델을 만든다</dd>
      <dd>학습률을 낮추면 비슷한 복잡도의 모델을 만들기 위해 더 많은 트리를 추가해야 한다</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><span style="color:red;font-weight:bold">n_estimators </span></dt>
      <dd>트리의 개수를 지정</dd>
      <dd>앙상블에 트리가 많이 추가되어 모델의 복잡도가 커지고 훈련세트의 오차를 바로잡을 기회가 많아짐</dd>
      <dd>estimators의 값이 클수록 모델이 복잡해지고 과대적합될 가능성이 있음!!!</dd>
    </dl>

    <p><strong>Tip!</strong> 가용한 시간과 메모리 한도에서 n_estimators를 맞추고 적절한 learning_rate를 찾는다</p>
  </li>
  <li>
    <dl>
      <dt><span style="color:red;font-weight:bold">n_iter_no_change, validation_fraction </span></dt>
      <dd>조기 종료를 위한 매개변수</dd>
      <dd>훈련 데이터에서 validation_fraction(기본값 0.1) 비율만큼 검증 데이터로 사용 -&gt; n_iter_no_change 반복 동안 검증 점수가 향상되지 않으면 훈련 종료</dd>
      <dd>n_iter_no_change 기본값이 None이면 조기 종료를 사용하지 않는다. <br />
<br />
<br /></dd>
    </dl>
  </li>
</ul>

<h2 id="237-배깅-엑스트라-트리-에이다부스트">2.3.7 배깅, 엑스트라 트리, 에이다부스트</h2>
<p>sckit-learn이 제공하는 다른 앙상블 알고리즘들!</p>

<h3 id="배깅">배깅</h3>
<p><strong>1) 배깅(Bootstrap aggregating)</strong></p>
<ul>
  <li>중복을 허용한 랜덤 샘플링으로 만든 훈련 세트를 사용하여 분류기에 각기 다르게 학습</li>
  <li>분류기가 predict_proba() 메서드를 지원 하면 -&gt; 확률값을 평균하여 예측 수행 <br />
지원하지 않으면 -&gt; 가장 빈도가 높은 클래스 레이블이 예측 결과가 됨</li>
</ul>

<p><strong>2) 실습</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span> <span class="c1">#로지스틱 회귀 모델을 100개 훈련해보겠음!
</span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                            <span class="n">oob_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># 로지스틱 회귀 객체를 기반 분류기(LogisticRegression())로 전달, 훈련할 분류기 개수는 100개(n_estimators=100)
# oob_score=True : 부트스트래핑에 포함되지 않은 샘플을 기반으로 훈련된 모델 평가, 테스트 세트 성능 짐작 가능!
</span><span class="n">bagging</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"훈련 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">bagging</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_train</span><span class="p">,</span> <span class="n">yc_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"테스트 세트 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">bagging</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xc_test</span><span class="p">,</span> <span class="n">yc_test</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"OOB 샘플의 정확도: {:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">bagging</span><span class="p">.</span><span class="n">oob_score_</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>훈련 세트 정확도: 0.953
테스트 세트 정확도: 0.951
OOB 샘플의 정확도: 0.946
</code></pre></div></div>

<h3 id="엑스트라-트리">엑스트라 트리</h3>
<p><strong>1) 엑스트라 트리(Extra-Trees)</strong></p>
<ul>
  <li>랜덤 포레스트와 비슷하지만 후보 특성을 무작위로 분할한 다음 최적의 분할을 찾는다.</li>
  <li>DecisionTreeClassifier(splitter=’random’)을 사용하고 부트스트랩 샘플링은 적용하지 않는다. (splitter=’random’은 무작위로 분할한 후보 노드 중 최선의 분할)</li>
  <li>다른 방식으로 모델에 무작위성을 주입!! 예측방식은 랜덤 포레스트와 동일하게 각 트리가 만든 확률값을 평균한다.</li>
</ul>

<p><strong>2) 엑스트라 트리 VS 랜덤 포레스트</strong> <br />
<img src="https://user-images.githubusercontent.com/63052097/135885304-222a9f52-daa7-4b7e-a326-fc1a8862a1ce.png" alt="image" />  <br />
[출처: <a href="https://wyatt37.tistory.com/6">끙정의 데이터 사이언스</a>]</p>

<p>랜덤 포레스트는 주어진 Feature에 대한 Partition을 계산하고 비교해서 최선의 Feature를 선택하여 Node를 분할   <br />
엑스트라 트리는 Feature 중에서 아무거나 고른 후에 Feature에 대해서만 최적의 분할을 찾아 Node를 분할</p>

<h3 id="에이다부스트">에이다부스트</h3>
<p><strong>1) 에이다부스트(Adaptive Boosting)</strong></p>
<ul>
  <li>훈련된 각 모델은 성능에 따라 가중치가 부여!</li>
  <li>그레이디언트 부스팅처럼 약한 학습기를 사용</li>
  <li>이전의 모델이 잘못 분류한 샘플에 가중치를 높여서 다음 모델을 훈련(그레디언트 부스팅과 차이)</li>
  <li>예측할 때는 모델이 예측한 레이블을 기준으로 모델의 가중치를 합산하여 가장 높은 값을 가진 레이블을 선택!</li>
  <li>AdaBoostClassifier의 기본값 = DecisionTreeClassifier(max_depth=1)를 사용 <br />
AdaBoostRegressor의 기본값 = DecisionTreeRegressor(max_depth=3)을 사용</li>
  <li>base_estimator 매개변수로 다른 모델을 지정할 수 있음!</li>
</ul>

<p>시간 관계상 늦게 필기를 할 것 같습니다.. 곧 업데이트 할 것이니 기다려주세요~ㅠㅠ</p>
<h2 id="238-커널-서포트-벡터-머신">2.3.8 커널 서포트 벡터 머신</h2>
<h3 id="선형-모델과-비선형-특성">선형 모델과 비선형 특성</h3>
<h3 id="커널-기법">커널 기법</h3>
<h3 id="svm-이해하기">SVM 이해하기</h3>
<h3 id="svm-매개변수-튜닝">SVM 매개변수 튜닝</h3>
<h3 id="svm을-위한-데이터-전처리">SVM을 위한 데이터 전처리</h3>
<h3 id="장단점과-매개변수-1">장단점과 매개변수</h3>

<h2 id="239신경망딥러닝">2.3.9신경망(딥러닝)</h2>
<h3 id="신경망-모델">신경망 모델</h3>
<h3 id="신경망-튜닝">신경망 튜닝</h3>
<h3 id="장단점과-매개변수-2">장단점과 매개변수</h3>
<h3 id="신경망의-복잡도-추정">신경망의 복잡도 추정</h3>

<h1 id="24-분류-예측의-불확실성-추정">2.4 분류 예측의 불확실성 추정</h1>
<h2 id="241-결정함수">2.4.1 결정함수</h2>
<h2 id="242-예측-확률">2.4.2 예측 확률</h2>
<h2 id="243-다중-분류에서의-불확실성">2.4.3 다중 분류에서의 불확실성</h2>

  </article>
  <br>

  <hr/><br>

  <div class="author">
    
    <table>
        <tr>
          <td rowspan="3" style="padding-right: 10px"><img class="author-pic" src="https://github.com/.png" alt=""></td>
          <td><b><h2 rel="author"></h2></b></td>
        </tr>
      <tr>
        <td><p rel="author"></p>
        </td>
      </tr>
      <tr>
        <td>
          <a rel="author" href="https://github.com/" target="_blank"><i class="fa fa-github fa-2x"></i></a>&nbsp&nbsp
          
        </td>
      </tr>
    </table>

  </div>


  <br>
  <hr/>
  <div>
    <script src="https://utteranc.es/client.js"
        repo="gdsc-seoultech/blog-comments"
        issue-term="pathname"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>
  </div>

</div>
<script src="/js/toc.js"></script>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper" align="center">
  	<h4>This site was built using <a href="http://jekyllrb.com" target="_blank">Jekyll</a> and is hosted on <a href="https://github.com" target="_blank">Github</a>. &#169; GDSC Seoultech</h4>
  </div>

</footer>


  </body>
</html>
